{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keerat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Downloading the inbuilt stop_words from nltk module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# stop_words is a list of all the INBUILT STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ['I', 'Me', 'My', 'Myself', 'We', 'Our', 'Ours', 'Ourselves', 'You', \"You're\", \"You've\", \"You'll\", \"You'd\", 'Your', \n",
    "        'Yours', 'Yourself', 'Yourselves', 'He', 'Him', 'His', 'Himself', 'She', \"She's\", 'Her', 'Hers', 'Herself', 'It', \n",
    "        \"It's\", 'Its', 'Itself', 'They', 'Them', 'Their', 'Theirs', 'Themselves', 'What', 'Which', 'Who', 'Whom', 'This', \n",
    "        'That', \"That'll\", 'These', 'Those', 'Am', 'Is', 'Are', 'Was', 'Were', 'Be', 'Been', 'Being', 'Have', 'Has', 'Had',\n",
    "        'Having', 'Do', 'Does', 'Did', 'Doing', 'A', 'An', 'The', 'And', 'But', 'If', 'Or', 'Because', 'As', 'Until', 'While',\n",
    "        'Of', 'At', 'By', 'For', 'With', 'About', 'Against', 'Between', 'Into', 'Through', 'During', 'Before', 'After', \n",
    "        'Above', 'Below', 'To', 'From', 'Up', 'Down', 'In', 'Out', 'On', 'Off', 'Over', 'Under', 'Again', 'Further', 'Then',\n",
    "        'Once', 'Here', 'There', 'When', 'Where', 'Why', 'How', 'All', 'Any', 'Both', 'Each', 'Few', 'More', 'Most', 'Other',\n",
    "        'Some', 'Such', 'No', 'Nor', 'Not', 'Only', 'Own', 'Same', 'So', 'Than', 'Too', 'very', 's', 't', 'Can', 'Will', \n",
    "        'Just', 'Don', \"Don't\", 'Should', \"Should've\", 'Now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'Ain', 'Aren', \"Aren't\",\n",
    "        'Couldn', \"Couldn't\", 'Didn', \"Didn't\", 'Doesn', \"Doesn't\", 'Hadn', \"Hadn't\", 'Hasn', \"Hasn't\", 'Haven', \"Haven't\",\n",
    "        'Isn', \"Isn't\", 'Ma', 'Mightn', \"Mightn't\", 'Mustn', \"Mustn't\", 'Needn', \"Needn't\", 'Shan', \"Shan't\", 'Shouldn', \n",
    "        \"Shouldn't\", 'Wasn', \"Wasn't\", 'Weren', \"Weren't\", 'Won', \"Won't\", 'Wouldn', \"Wouldn't\",\"can't\",\"Can't\",'could',\n",
    "        'Could','would','Would']\n",
    "\n",
    "# l1 is a list which comprises of a few more stopwords and \n",
    "# all those words in the stop_word list above only (But with all their initials capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'I',\n",
       " 'Me',\n",
       " 'My',\n",
       " 'Myself',\n",
       " 'We',\n",
       " 'Our',\n",
       " 'Ours',\n",
       " 'Ourselves',\n",
       " 'You',\n",
       " \"You're\",\n",
       " \"You've\",\n",
       " \"You'll\",\n",
       " \"You'd\",\n",
       " 'Your',\n",
       " 'Yours',\n",
       " 'Yourself',\n",
       " 'Yourselves',\n",
       " 'He',\n",
       " 'Him',\n",
       " 'His',\n",
       " 'Himself',\n",
       " 'She',\n",
       " \"She's\",\n",
       " 'Her',\n",
       " 'Hers',\n",
       " 'Herself',\n",
       " 'It',\n",
       " \"It's\",\n",
       " 'Its',\n",
       " 'Itself',\n",
       " 'They',\n",
       " 'Them',\n",
       " 'Their',\n",
       " 'Theirs',\n",
       " 'Themselves',\n",
       " 'What',\n",
       " 'Which',\n",
       " 'Who',\n",
       " 'Whom',\n",
       " 'This',\n",
       " 'That',\n",
       " \"That'll\",\n",
       " 'These',\n",
       " 'Those',\n",
       " 'Am',\n",
       " 'Is',\n",
       " 'Are',\n",
       " 'Was',\n",
       " 'Were',\n",
       " 'Be',\n",
       " 'Been',\n",
       " 'Being',\n",
       " 'Have',\n",
       " 'Has',\n",
       " 'Had',\n",
       " 'Having',\n",
       " 'Do',\n",
       " 'Does',\n",
       " 'Did',\n",
       " 'Doing',\n",
       " 'A',\n",
       " 'An',\n",
       " 'The',\n",
       " 'And',\n",
       " 'But',\n",
       " 'If',\n",
       " 'Or',\n",
       " 'Because',\n",
       " 'As',\n",
       " 'Until',\n",
       " 'While',\n",
       " 'Of',\n",
       " 'At',\n",
       " 'By',\n",
       " 'For',\n",
       " 'With',\n",
       " 'About',\n",
       " 'Against',\n",
       " 'Between',\n",
       " 'Into',\n",
       " 'Through',\n",
       " 'During',\n",
       " 'Before',\n",
       " 'After',\n",
       " 'Above',\n",
       " 'Below',\n",
       " 'To',\n",
       " 'From',\n",
       " 'Up',\n",
       " 'Down',\n",
       " 'In',\n",
       " 'Out',\n",
       " 'On',\n",
       " 'Off',\n",
       " 'Over',\n",
       " 'Under',\n",
       " 'Again',\n",
       " 'Further',\n",
       " 'Then',\n",
       " 'Once',\n",
       " 'Here',\n",
       " 'There',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Why',\n",
       " 'How',\n",
       " 'All',\n",
       " 'Any',\n",
       " 'Both',\n",
       " 'Each',\n",
       " 'Few',\n",
       " 'More',\n",
       " 'Most',\n",
       " 'Other',\n",
       " 'Some',\n",
       " 'Such',\n",
       " 'No',\n",
       " 'Nor',\n",
       " 'Not',\n",
       " 'Only',\n",
       " 'Own',\n",
       " 'Same',\n",
       " 'So',\n",
       " 'Than',\n",
       " 'Too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'Can',\n",
       " 'Will',\n",
       " 'Just',\n",
       " 'Don',\n",
       " \"Don't\",\n",
       " 'Should',\n",
       " \"Should've\",\n",
       " 'Now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'Ain',\n",
       " 'Aren',\n",
       " \"Aren't\",\n",
       " 'Couldn',\n",
       " \"Couldn't\",\n",
       " 'Didn',\n",
       " \"Didn't\",\n",
       " 'Doesn',\n",
       " \"Doesn't\",\n",
       " 'Hadn',\n",
       " \"Hadn't\",\n",
       " 'Hasn',\n",
       " \"Hasn't\",\n",
       " 'Haven',\n",
       " \"Haven't\",\n",
       " 'Isn',\n",
       " \"Isn't\",\n",
       " 'Ma',\n",
       " 'Mightn',\n",
       " \"Mightn't\",\n",
       " 'Mustn',\n",
       " \"Mustn't\",\n",
       " 'Needn',\n",
       " \"Needn't\",\n",
       " 'Shan',\n",
       " \"Shan't\",\n",
       " 'Shouldn',\n",
       " \"Shouldn't\",\n",
       " 'Wasn',\n",
       " \"Wasn't\",\n",
       " 'Weren',\n",
       " \"Weren't\",\n",
       " 'Won',\n",
       " \"Won't\",\n",
       " 'Wouldn',\n",
       " \"Wouldn't\",\n",
       " \"can't\",\n",
       " \"Can't\",\n",
       " 'could',\n",
       " 'Could',\n",
       " 'would',\n",
       " 'Would',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '100',\n",
       " '/*',\n",
       " '*/',\n",
       " '/',\n",
       " ':',\n",
       " '=',\n",
       " '|>',\n",
       " '<',\n",
       " '>',\n",
       " '@',\n",
       " '#',\n",
       " '-',\n",
       " '|',\n",
       " '--',\n",
       " '<<',\n",
       " '>>',\n",
       " '+',\n",
       " '*',\n",
       " '{',\n",
       " '}',\n",
       " '$',\n",
       " '$2',\n",
       " '$1',\n",
       " '?',\n",
       " '_',\n",
       " '_/',\n",
       " 'Message-ID:',\n",
       " 'Date:',\n",
       " '1993',\n",
       " 'Path:',\n",
       " 'From:',\n",
       " 'GMT',\n",
       " 'Subject:',\n",
       " 'Sender:',\n",
       " 'References:',\n",
       " 'Re:',\n",
       " 'Newsgroups:',\n",
       " 'Xref:',\n",
       " 'Organization:',\n",
       " 'Lines:',\n",
       " 'writes:',\n",
       " 'Jan',\n",
       " 'Feb',\n",
       " 'Mar',\n",
       " 'Apr',\n",
       " 'May',\n",
       " 'Jun',\n",
       " 'Jul',\n",
       " 'Aug',\n",
       " 'Sep',\n",
       " 'Oct',\n",
       " 'Nov',\n",
       " 'Dec',\n",
       " 'Mon,',\n",
       " 'Tue,',\n",
       " 'Wed,',\n",
       " 'Thu,',\n",
       " 'Fri,',\n",
       " 'Sat,',\n",
       " 'Sun,',\n",
       " \"MAX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>\",\n",
       " '>I',\n",
       " \"I've\",\n",
       " \"MAX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'\",\n",
       " 'it,',\n",
       " 'it.',\n",
       " \"that's\",\n",
       " '#>',\n",
       " '#|>',\n",
       " 'Reply-To:',\n",
       " '<<<',\n",
       " '>>>',\n",
       " '>|>',\n",
       " '&',\n",
       " 'Distribution:',\n",
       " \"I'm\",\n",
       " '\\\\',\n",
       " 'Keywords:',\n",
       " '(1st',\n",
       " 'appears)',\n",
       " '$3',\n",
       " '>In',\n",
       " \"I'd\",\n",
       " 'many',\n",
       " 'Reply-To:',\n",
       " '>|>',\n",
       " '...',\n",
       " '---',\n",
       " '!',\n",
       " '**',\n",
       " '(--)',\n",
       " '.',\n",
       " '(',\n",
       " ')',\n",
       " '----------------------------------------------------------------------------',\n",
       " '',\n",
       " '--',\n",
       " 'Followup-To:',\n",
       " 'it?',\n",
       " '>and',\n",
       " '>The',\n",
       " '>to',\n",
       " 'So,',\n",
       " ':-)',\n",
       " \"I'll\",\n",
       " 'you,',\n",
       " 'Yet',\n",
       " '[',\n",
       " ']',\n",
       " '->',\n",
       " '<?>',\n",
       " '***',\n",
       " 'M\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(',\n",
       " '{>',\n",
       " '>the',\n",
       " '==',\n",
       " '#1',\n",
       " '%',\n",
       " '/|',\n",
       " '(I',\n",
       " '>:',\n",
       " 'ED>',\n",
       " '[A]',\n",
       " 'FAQ:',\n",
       " '\"A',\n",
       " '\"What',\n",
       " 'to.',\n",
       " '\"I',\n",
       " 'is.',\n",
       " 'No,',\n",
       " 'have.',\n",
       " 'good.>This',\n",
       " '>>I',\n",
       " '>Well,',\n",
       " '>And',\n",
       " '>a',\n",
       " '_The',\n",
       " \"they're\",\n",
       " 'this?',\n",
       " '\"',\n",
       " \">I'm\",\n",
       " 'here.',\n",
       " '>that',\n",
       " '>of',\n",
       " '>it',\n",
       " '\"not',\n",
       " ':)',\n",
       " '[...]',\n",
       " '>in',\n",
       " 'not?',\n",
       " '>You',\n",
       " '>If',\n",
       " '/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\',\n",
       " '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
       " '>>to',\n",
       " '>--',\n",
       " '>As',\n",
       " '>this',\n",
       " '_not_',\n",
       " '>is',\n",
       " '>>the',\n",
       " '>What',\n",
       " '>you',\n",
       " 'do,',\n",
       " '(was',\n",
       " \"we're\",\n",
       " 'yet',\n",
       " 'too.',\n",
       " 'them?',\n",
       " '(as',\n",
       " 'right?',\n",
       " '(as',\n",
       " 'will.',\n",
       " 'him,',\n",
       " '>This',\n",
       " \"That's\",\n",
       " 'much',\n",
       " 'although',\n",
       " 'shall',\n",
       " '\"If',\n",
       " '(or',\n",
       " '(and',\n",
       " 'and/or',\n",
       " 'Without',\n",
       " 'goes',\n",
       " 'must',\n",
       " 'often',\n",
       " 'A.',\n",
       " 'B.',\n",
       " 'C.',\n",
       " 'D.',\n",
       " 'E.',\n",
       " 'F.',\n",
       " 'G.',\n",
       " 'H.',\n",
       " 'I.',\n",
       " 'J.',\n",
       " 'K.',\n",
       " 'L.',\n",
       " 'M.',\n",
       " 'N.',\n",
       " 'O.',\n",
       " 'P.',\n",
       " 'Q.',\n",
       " 'R.',\n",
       " 'S.',\n",
       " 'T.',\n",
       " '\"The',\n",
       " 'on.',\n",
       " 'may',\n",
       " 'FOR',\n",
       " 'and/or',\n",
       " 'Without',\n",
       " 'is,',\n",
       " 'also',\n",
       " 'not.',\n",
       " 'do.',\n",
       " 'though',\n",
       " 'therefore',\n",
       " \"there's\",\n",
       " '\"the',\n",
       " 'B',\n",
       " '\"If',\n",
       " 'shall',\n",
       " \"That's\",\n",
       " '>This',\n",
       " 'me,',\n",
       " 'that,',\n",
       " '----------------------------------------------------------------------',\n",
       " '1993',\n",
       " '----']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [str(i) for i in range(101)]\n",
    "\n",
    "# List of '0' ------ '100' as they are also useless for becoming features\n",
    "\n",
    "\n",
    "l3 = [\"/*\",'*/','/',':','=','|>','<','>','@','#','-','|','--','<<','>>','+','*','{','}','$','$2', '$1','?','_','_/',\n",
    "        'Message-ID:','Date:','1993','Path:','From:','GMT','Subject:','Sender:','References:',\n",
    "        'Re:','Newsgroups:','Xref:','Organization:','Lines:','writes:',\n",
    "        'Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec','Mon,','Tue,','Wed,','Thu,','Fri,','Sat,',\n",
    "        'Sun,',\n",
    "        \"MAX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>\",'>I',\"I've\",\n",
    "        \"MAX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'AX>'\",'it,','it.',\"that's\", '#>','#|>','Reply-To:','<<<',\n",
    "        '>>>', '>|>','&','Distribution:',\"I'm\",'\\\\','Keywords:','(1st','appears)','$3','>In',\n",
    "        \"I'd\",'many','Reply-To:','>|>','...','---','!','**','(--)','.','(',')',\n",
    "        '----------------------------------------------------------------------------','','--',\n",
    "        'Followup-To:','it?','>and','>The','>to','So,',':-)',\"I'll\",'you,','Yet','[',']','->','<?>','***',\n",
    "        'M\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(','{>',\n",
    "        '>the','==','#1','%','/|','(I','>:','ED>','[A]',\n",
    "        'FAQ:','\"A','\"What','to.','\"I','is.','No,','have.','good.''>This','>>I','>Well,','>And','>a','_The',\"they're\",\n",
    "        'this?','\"',\">I'm\",'here.','>that','>of','>it','\"not',':)','[...]','>in','not?','>You','>If',\n",
    "        '/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\',\n",
    "        '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
    "        '>>to','>--','>As','>this','_not_','>is','>>the',\n",
    "        '>What','>you','do,','(was',\"we're\",'yet','too.','them?','(as','right?','(as','will.','him,',\n",
    "        '>This',\"That's\",'much','although','shall','\"If','(or','(and','and/or','Without','goes','must','often',\n",
    "        'A.','B.','C.','D.','E.','F.','G.','H.','I.','J.','K.','L.','M.','N.','O.','P.','Q.','R.','S.','T.',\n",
    "        '\"The','on.','may','FOR','and/or','Without','is,','also','not.','do.','though','therefore',\"there's\",\n",
    "        '\"the','B','\"If','shall',\"That's\",'>This','me,','that,',\n",
    "        '****************************************************************',\n",
    "        '----------------------------------------------------------------------','1993','----']\n",
    "\n",
    "\n",
    "# l3 is a list of some useless words and symbols which i came accross in the files\n",
    "\n",
    "\n",
    "\n",
    "# All the above three lists are added to the stop_words list defined above \n",
    "\n",
    "stop_words.extend(l1)\n",
    "stop_words.extend(l2)\n",
    "stop_words.extend(l3)\n",
    "\n",
    "\n",
    "# stop_words                # This is the list of all possible stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE :\n",
    "I tried to implement something like this:\n",
    "\n",
    "line = re.sub(\"[^a-zA-Z ]\",\" \",line) \n",
    "and word = word.lower()                \n",
    "\n",
    "also\n",
    "\n",
    "i.e replacing all the numbers and other special characters by a space and making each word in Lower Case so that the size of the list of stop_words could be reduced and we could achieve a better accuracy\n",
    "\n",
    "But the maximum accuracy is achieved through this particular code only.\n",
    "I have also attested the code with such implementation below.\n",
    "You can see and compare both.That code starts from cell number 8 (FROM BOTTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism - comp.graphics - comp.os.ms-windows.misc - comp.sys.ibm.pc.hardware - comp.sys.mac.hardware - comp.windows.x - misc.forsale - rec.autos - rec.motorcycles - rec.sport.baseball - rec.sport.hockey - sci.crypt - sci.electronics - sci.med - sci.space - soc.religion.christian - talk.politics.guns - talk.politics.mideast - talk.politics.misc - talk.religion.misc - "
     ]
    }
   ],
   "source": [
    "path_20_newsgrp = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Training data)/20_newsgroups'\n",
    "\n",
    "# This is the path of the root folder for the (TRAINING DATA)\n",
    "# (Which contains all the other 20 folders of different classes)\n",
    "# Each of these 20 folders comprises of all the files or the documents\n",
    "# The total number of files are 19,997\n",
    "\n",
    "\n",
    "categories = os.listdir(path_20_newsgrp)    # A list of all the class_types\n",
    "# The above list(categories) comprises of all the 20 folders' names which are as:\n",
    "\n",
    "for category in categories:\n",
    "    print(category,end = ' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line  1  :  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49960 alt.atheism.moderated:713 news.answers:7054 alt.answers:126\n",
      "\n",
      "Words in Line  1  :  ['Xref:', 'cantaloupe.srv.cs.cmu.edu', 'alt.atheism:49960', 'alt.atheism.moderated:713', 'news.answers:7054', 'alt.answers:126']\n",
      "\n",
      "Line  2  :  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew\n",
      "\n",
      "Words in Line  2  :  ['Path:', 'cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew']\n",
      "\n",
      "Line  3  :  From: mathew <mathew@mantis.co.uk>\n",
      "\n",
      "Words in Line  3  :  ['From:', 'mathew', '<mathew@mantis.co.uk>']\n",
      "\n",
      "Line  4  :  Newsgroups: alt.atheism,alt.atheism.moderated,news.answers,alt.answers\n",
      "\n",
      "Words in Line  4  :  ['Newsgroups:', 'alt.atheism,alt.atheism.moderated,news.answers,alt.answers']\n",
      "\n",
      "Line  5  :  Subject: Alt.Atheism FAQ: Atheist Resources\n",
      "\n",
      "Words in Line  5  :  ['Subject:', 'Alt.Atheism', 'FAQ:', 'Atheist', 'Resources']\n",
      "\n",
      "Line  6  :  Summary: Books, addresses, music -- anything related to atheism\n",
      "\n",
      "Words in Line  6  :  ['Summary:', 'Books,', 'addresses,', 'music', '--', 'anything', 'related', 'to', 'atheism']\n",
      "\n",
      "Line  7  :  Keywords: FAQ, atheism, books, music, fiction, addresses, contacts\n",
      "\n",
      "Words in Line  7  :  ['Keywords:', 'FAQ,', 'atheism,', 'books,', 'music,', 'fiction,', 'addresses,', 'contacts']\n",
      "\n",
      "Line  8  :  Message-ID: <19930329115719@mantis.co.uk>\n",
      "\n",
      "Words in Line  8  :  ['Message-ID:', '<19930329115719@mantis.co.uk>']\n",
      "\n",
      "Line  9  :  Date: Mon, 29 Mar 1993 11:57:19 GMT\n",
      "\n",
      "Words in Line  9  :  ['Date:', 'Mon,', '29', 'Mar', '1993', '11:57:19', 'GMT']\n",
      "\n",
      "Line  10  :  Expires: Thu, 29 Apr 1993 11:57:19 GMT\n",
      "\n",
      "Words in Line  10  :  ['Expires:', 'Thu,', '29', 'Apr', '1993', '11:57:19', 'GMT']\n",
      "\n",
      "Line  11  :  Followup-To: alt.atheism\n",
      "\n",
      "Words in Line  11  :  ['Followup-To:', 'alt.atheism']\n",
      "\n",
      "Line  12  :  Distribution: world\n",
      "\n",
      "Words in Line  12  :  ['Distribution:', 'world']\n",
      "\n",
      "Line  13  :  Organization: Mantis Consultants, Cambridge. UK.\n",
      "\n",
      "Words in Line  13  :  ['Organization:', 'Mantis', 'Consultants,', 'Cambridge.', 'UK.']\n",
      "\n",
      "Line  14  :  Approved: news-answers-request@mit.edu\n",
      "\n",
      "Words in Line  14  :  ['Approved:', 'news-answers-request@mit.edu']\n",
      "\n",
      "Line  15  :  Supersedes: <19930301143317@mantis.co.uk>\n",
      "\n",
      "Words in Line  15  :  ['Supersedes:', '<19930301143317@mantis.co.uk>']\n",
      "\n",
      "Line  16  :  Lines: 290\n",
      "\n",
      "Words in Line  16  :  ['Lines:', '290']\n",
      "\n",
      "Line  17  :  \n",
      "\n",
      "Words in Line  17  :  ['']\n",
      "\n",
      "Line  18  :  Archive-name: atheism/resources\n",
      "\n",
      "Words in Line  18  :  ['Archive-name:', 'atheism/resources']\n",
      "\n",
      "Line  19  :  Alt-atheism-archive-name: resources\n",
      "\n",
      "Words in Line  19  :  ['Alt-atheism-archive-name:', 'resources']\n",
      "\n",
      "Line  20  :  Last-modified: 11 December 1992\n",
      "\n",
      "Words in Line  20  :  ['Last-modified:', '11', 'December', '1992']\n",
      "\n",
      "Line  21  :  Version: 1.0\n",
      "\n",
      "Words in Line  21  :  ['Version:', '1.0']\n",
      "\n",
      "Line  22  :  \n",
      "\n",
      "Words in Line  22  :  ['']\n",
      "\n",
      "Line  23  :                                Atheist Resources\n",
      "\n",
      "Words in Line  23  :  ['Atheist', 'Resources']\n",
      "\n",
      "Line  24  :  \n",
      "\n",
      "Words in Line  24  :  ['']\n",
      "\n",
      "Line  25  :                        Addresses of Atheist Organizations\n",
      "\n",
      "Words in Line  25  :  ['Addresses', 'of', 'Atheist', 'Organizations']\n",
      "\n",
      "Line  26  :  \n",
      "\n",
      "Words in Line  26  :  ['']\n",
      "\n",
      "Line  27  :                                       USA\n",
      "\n",
      "Words in Line  27  :  ['USA']\n",
      "\n",
      "Line  28  :  \n",
      "\n",
      "Words in Line  28  :  ['']\n",
      "\n",
      "Line  29  :  FREEDOM FROM RELIGION FOUNDATION\n",
      "\n",
      "Words in Line  29  :  ['FREEDOM', 'FROM', 'RELIGION', 'FOUNDATION']\n",
      "\n",
      "Line  30  :  \n",
      "\n",
      "Words in Line  30  :  ['']\n",
      "\n",
      "Line  31  :  Darwin fish bumper stickers and assorted other atheist paraphernalia are\n",
      "\n",
      "Words in Line  31  :  ['Darwin', 'fish', 'bumper', 'stickers', 'and', 'assorted', 'other', 'atheist', 'paraphernalia', 'are']\n",
      "\n",
      "Line  32  :  available from the Freedom From Religion Foundation in the US.\n",
      "\n",
      "Words in Line  32  :  ['available', 'from', 'the', 'Freedom', 'From', 'Religion', 'Foundation', 'in', 'the', 'US.']\n",
      "\n",
      "Line  33  :  \n",
      "\n",
      "Words in Line  33  :  ['']\n",
      "\n",
      "Line  34  :  Write to:  FFRF, P.O. Box 750, Madison, WI 53701.\n",
      "\n",
      "Words in Line  34  :  ['Write', 'to:', '', 'FFRF,', 'P.O.', 'Box', '750,', 'Madison,', 'WI', '53701.']\n",
      "\n",
      "Line  35  :  Telephone: (608) 256-8900\n",
      "\n",
      "Words in Line  35  :  ['Telephone:', '(608)', '256-8900']\n",
      "\n",
      "Line  36  :  \n",
      "\n",
      "Words in Line  36  :  ['']\n",
      "\n",
      "Line  37  :  EVOLUTION DESIGNS\n",
      "\n",
      "Words in Line  37  :  ['EVOLUTION', 'DESIGNS']\n",
      "\n",
      "Line  38  :  \n",
      "\n",
      "Words in Line  38  :  ['']\n",
      "\n",
      "Line  39  :  Evolution Designs sell the \"Darwin fish\".  It's a fish symbol, like the ones\n",
      "\n",
      "Words in Line  39  :  ['Evolution', 'Designs', 'sell', 'the', '\"Darwin', 'fish\".', '', \"It's\", 'a', 'fish', 'symbol,', 'like', 'the', 'ones']\n",
      "\n",
      "Line  40  :  Christians stick on their cars, but with feet and the word \"Darwin\" written\n",
      "\n",
      "Words in Line  40  :  ['Christians', 'stick', 'on', 'their', 'cars,', 'but', 'with', 'feet', 'and', 'the', 'word', '\"Darwin\"', 'written']\n",
      "\n",
      "Line  41  :  inside.  The deluxe moulded 3D plastic fish is $4.95 postpaid in the US.\n",
      "\n",
      "Words in Line  41  :  ['inside.', '', 'The', 'deluxe', 'moulded', '3D', 'plastic', 'fish', 'is', '$4.95', 'postpaid', 'in', 'the', 'US.']\n",
      "\n",
      "Line  42  :  \n",
      "\n",
      "Words in Line  42  :  ['']\n",
      "\n",
      "Line  43  :  Write to:  Evolution Designs, 7119 Laurel Canyon #4, North Hollywood,\n",
      "\n",
      "Words in Line  43  :  ['Write', 'to:', '', 'Evolution', 'Designs,', '7119', 'Laurel', 'Canyon', '#4,', 'North', 'Hollywood,']\n",
      "\n",
      "Line  44  :             CA 91605.\n",
      "\n",
      "Words in Line  44  :  ['CA', '91605.']\n",
      "\n",
      "Line  45  :  \n",
      "\n",
      "Words in Line  45  :  ['']\n",
      "\n",
      "Line  46  :  People in the San Francisco Bay area can get Darwin Fish from Lynn Gold --\n",
      "\n",
      "Words in Line  46  :  ['People', 'in', 'the', 'San', 'Francisco', 'Bay', 'area', 'can', 'get', 'Darwin', 'Fish', 'from', 'Lynn', 'Gold', '--']\n",
      "\n",
      "Line  47  :  try mailing <figmo@netcom.com>.  For net people who go to Lynn directly, the\n",
      "\n",
      "Words in Line  47  :  ['try', 'mailing', '<figmo@netcom.com>.', '', 'For', 'net', 'people', 'who', 'go', 'to', 'Lynn', 'directly,', 'the']\n",
      "\n",
      "Line  48  :  price is $4.95 per fish.\n",
      "\n",
      "Words in Line  48  :  ['price', 'is', '$4.95', 'per', 'fish.']\n",
      "\n",
      "Line  49  :  \n",
      "\n",
      "Words in Line  49  :  ['']\n",
      "\n",
      "Line  50  :  AMERICAN ATHEIST PRESS\n",
      "\n",
      "Words in Line  50  :  ['AMERICAN', 'ATHEIST', 'PRESS']\n",
      "\n",
      "Line  51  :  \n",
      "\n",
      "Words in Line  51  :  ['']\n",
      "\n",
      "Line  52  :  AAP publish various atheist books -- critiques of the Bible, lists of\n",
      "\n",
      "Words in Line  52  :  ['AAP', 'publish', 'various', 'atheist', 'books', '--', 'critiques', 'of', 'the', 'Bible,', 'lists', 'of']\n",
      "\n",
      "Line  53  :  Biblical contradictions, and so on.  One such book is:\n",
      "\n",
      "Words in Line  53  :  ['Biblical', 'contradictions,', 'and', 'so', 'on.', '', 'One', 'such', 'book', 'is:']\n",
      "\n",
      "Line  54  :  \n",
      "\n",
      "Words in Line  54  :  ['']\n",
      "\n",
      "Line  55  :  \"The Bible Handbook\" by W.P. Ball and G.W. Foote.  American Atheist Press.\n",
      "\n",
      "Words in Line  55  :  ['\"The', 'Bible', 'Handbook\"', 'by', 'W.P.', 'Ball', 'and', 'G.W.', 'Foote.', '', 'American', 'Atheist', 'Press.']\n",
      "\n",
      "Line  56  :  372 pp.  ISBN 0-910309-26-4, 2nd edition, 1986.  Bible contradictions,\n",
      "\n",
      "Words in Line  56  :  ['372', 'pp.', '', 'ISBN', '0-910309-26-4,', '2nd', 'edition,', '1986.', '', 'Bible', 'contradictions,']\n",
      "\n",
      "Line  57  :  absurdities, atrocities, immoralities... contains Ball, Foote: \"The Bible\n",
      "\n",
      "Words in Line  57  :  ['absurdities,', 'atrocities,', 'immoralities...', 'contains', 'Ball,', 'Foote:', '\"The', 'Bible']\n",
      "\n",
      "Line  58  :  Contradicts Itself\", AAP.  Based on the King James version of the Bible.\n",
      "\n",
      "Words in Line  58  :  ['Contradicts', 'Itself\",', 'AAP.', '', 'Based', 'on', 'the', 'King', 'James', 'version', 'of', 'the', 'Bible.']\n",
      "\n",
      "Line  59  :  \n",
      "\n",
      "Words in Line  59  :  ['']\n",
      "\n",
      "Line  60  :  Write to:  American Atheist Press, P.O. Box 140195, Austin, TX 78714-0195.\n",
      "\n",
      "Words in Line  60  :  ['Write', 'to:', '', 'American', 'Atheist', 'Press,', 'P.O.', 'Box', '140195,', 'Austin,', 'TX', '78714-0195.']\n",
      "\n",
      "Line  61  :        or:  7215 Cameron Road, Austin, TX 78752-2973.\n",
      "\n",
      "Words in Line  61  :  ['or:', '', '7215', 'Cameron', 'Road,', 'Austin,', 'TX', '78752-2973.']\n",
      "\n",
      "Line  62  :  Telephone: (512) 458-1244\n",
      "\n",
      "Words in Line  62  :  ['Telephone:', '(512)', '458-1244']\n",
      "\n",
      "Line  63  :  Fax:       (512) 467-9525\n",
      "\n",
      "Words in Line  63  :  ['Fax:', '', '', '', '', '', '', '(512)', '467-9525']\n",
      "\n",
      "Line  64  :  \n",
      "\n",
      "Words in Line  64  :  ['']\n",
      "\n",
      "Line  65  :  PROMETHEUS BOOKS\n",
      "\n",
      "Words in Line  65  :  ['PROMETHEUS', 'BOOKS']\n",
      "\n",
      "Line  66  :  \n",
      "\n",
      "Words in Line  66  :  ['']\n",
      "\n",
      "Line  67  :  Sell books including Haught's \"Holy Horrors\" (see below).\n",
      "\n",
      "Words in Line  67  :  ['Sell', 'books', 'including', \"Haught's\", '\"Holy', 'Horrors\"', '(see', 'below).']\n",
      "\n",
      "Line  68  :  \n",
      "\n",
      "Words in Line  68  :  ['']\n",
      "\n",
      "Line  69  :  Write to:  700 East Amherst Street, Buffalo, New York 14215.\n",
      "\n",
      "Words in Line  69  :  ['Write', 'to:', '', '700', 'East', 'Amherst', 'Street,', 'Buffalo,', 'New', 'York', '14215.']\n",
      "\n",
      "Line  70  :  Telephone: (716) 837-2475.\n",
      "\n",
      "Words in Line  70  :  ['Telephone:', '(716)', '837-2475.']\n",
      "\n",
      "Line  71  :  \n",
      "\n",
      "Words in Line  71  :  ['']\n",
      "\n",
      "Line  72  :  An alternate address (which may be newer or older) is:\n",
      "\n",
      "Words in Line  72  :  ['An', 'alternate', 'address', '(which', 'may', 'be', 'newer', 'or', 'older)', 'is:']\n",
      "\n",
      "Line  73  :  Prometheus Books, 59 Glenn Drive, Buffalo, NY 14228-2197.\n",
      "\n",
      "Words in Line  73  :  ['Prometheus', 'Books,', '59', 'Glenn', 'Drive,', 'Buffalo,', 'NY', '14228-2197.']\n",
      "\n",
      "Line  74  :  \n",
      "\n",
      "Words in Line  74  :  ['']\n",
      "\n",
      "Line  75  :  AFRICAN-AMERICANS FOR HUMANISM\n",
      "\n",
      "Words in Line  75  :  ['AFRICAN-AMERICANS', 'FOR', 'HUMANISM']\n",
      "\n",
      "Line  76  :  \n",
      "\n",
      "Words in Line  76  :  ['']\n",
      "\n",
      "Line  77  :  An organization promoting black secular humanism and uncovering the history of\n",
      "\n",
      "Words in Line  77  :  ['An', 'organization', 'promoting', 'black', 'secular', 'humanism', 'and', 'uncovering', 'the', 'history', 'of']\n",
      "\n",
      "Line  78  :  black freethought.  They publish a quarterly newsletter, AAH EXAMINER.\n",
      "\n",
      "Words in Line  78  :  ['black', 'freethought.', '', 'They', 'publish', 'a', 'quarterly', 'newsletter,', 'AAH', 'EXAMINER.']\n",
      "\n",
      "Line  79  :  \n",
      "\n",
      "Words in Line  79  :  ['']\n",
      "\n",
      "Line  80  :  Write to:  Norm R. Allen, Jr., African Americans for Humanism, P.O. Box 664,\n",
      "\n",
      "Words in Line  80  :  ['Write', 'to:', '', 'Norm', 'R.', 'Allen,', 'Jr.,', 'African', 'Americans', 'for', 'Humanism,', 'P.O.', 'Box', '664,']\n",
      "\n",
      "Line  81  :             Buffalo, NY 14226.\n",
      "\n",
      "Words in Line  81  :  ['Buffalo,', 'NY', '14226.']\n",
      "\n",
      "Line  82  :  \n",
      "\n",
      "Words in Line  82  :  ['']\n",
      "\n",
      "Line  83  :                                  United Kingdom\n",
      "\n",
      "Words in Line  83  :  ['United', 'Kingdom']\n",
      "\n",
      "Line  84  :  \n",
      "\n",
      "Words in Line  84  :  ['']\n",
      "\n",
      "Line  85  :  Rationalist Press Association          National Secular Society\n",
      "\n",
      "Words in Line  85  :  ['Rationalist', 'Press', 'Association', '', '', '', '', '', '', '', '', '', 'National', 'Secular', 'Society']\n",
      "\n",
      "Line  86  :  88 Islington High Street               702 Holloway Road\n",
      "\n",
      "Words in Line  86  :  ['88', 'Islington', 'High', 'Street', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '702', 'Holloway', 'Road']\n",
      "\n",
      "Line  87  :  London N1 8EW                          London N19 3NL\n",
      "\n",
      "Words in Line  87  :  ['London', 'N1', '8EW', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'London', 'N19', '3NL']\n",
      "\n",
      "Line  88  :  071 226 7251                           071 272 1266\n",
      "\n",
      "Words in Line  88  :  ['071', '226', '7251', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '071', '272', '1266']\n",
      "\n",
      "Line  89  :  \n",
      "\n",
      "Words in Line  89  :  ['']\n",
      "\n",
      "Line  90  :  British Humanist Association           South Place Ethical Society\n",
      "\n",
      "Words in Line  90  :  ['British', 'Humanist', 'Association', '', '', '', '', '', '', '', '', '', '', 'South', 'Place', 'Ethical', 'Society']\n",
      "\n",
      "Line  91  :  14 Lamb's Conduit Passage              Conway Hall\n",
      "\n",
      "Words in Line  91  :  ['14', \"Lamb's\", 'Conduit', 'Passage', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Conway', 'Hall']\n",
      "\n",
      "Line  92  :  London WC1R 4RH                        Red Lion Square\n",
      "\n",
      "Words in Line  92  :  ['London', 'WC1R', '4RH', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Red', 'Lion', 'Square']\n",
      "\n",
      "Line  93  :  071 430 0908                           London WC1R 4RL\n",
      "\n",
      "Words in Line  93  :  ['071', '430', '0908', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'London', 'WC1R', '4RL']\n",
      "\n",
      "Line  94  :  fax 071 430 1271                       071 831 7723\n",
      "\n",
      "Words in Line  94  :  ['fax', '071', '430', '1271', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '071', '831', '7723']\n",
      "\n",
      "Line  95  :  \n",
      "\n",
      "Words in Line  95  :  ['']\n",
      "\n",
      "Line  96  :  The National Secular Society publish \"The Freethinker\", a monthly magazine\n",
      "\n",
      "Words in Line  96  :  ['The', 'National', 'Secular', 'Society', 'publish', '\"The', 'Freethinker\",', 'a', 'monthly', 'magazine']\n",
      "\n",
      "Line  97  :  founded in 1881.\n",
      "\n",
      "Words in Line  97  :  ['founded', 'in', '1881.']\n",
      "\n",
      "Line  98  :  \n",
      "\n",
      "Words in Line  98  :  ['']\n",
      "\n",
      "Line  99  :                                     Germany\n",
      "\n",
      "Words in Line  99  :  ['Germany']\n",
      "\n",
      "Line  100  :  \n",
      "\n",
      "Words in Line  100  :  ['']\n",
      "\n",
      "Line  101  :  IBKA e.V.\n",
      "\n",
      "Words in Line  101  :  ['IBKA', 'e.V.']\n",
      "\n",
      "Line  102  :  Internationaler Bund der Konfessionslosen und Atheisten\n",
      "\n",
      "Words in Line  102  :  ['Internationaler', 'Bund', 'der', 'Konfessionslosen', 'und', 'Atheisten']\n",
      "\n",
      "Line  103  :  Postfach 880, D-1000 Berlin 41. Germany.\n",
      "\n",
      "Words in Line  103  :  ['Postfach', '880,', 'D-1000', 'Berlin', '41.', 'Germany.']\n",
      "\n",
      "Line  104  :  \n",
      "\n",
      "Words in Line  104  :  ['']\n",
      "\n",
      "Line  105  :  IBKA publish a journal:\n",
      "\n",
      "Words in Line  105  :  ['IBKA', 'publish', 'a', 'journal:']\n",
      "\n",
      "Line  106  :  MIZ. (Materialien und Informationen zur Zeit. Politisches\n",
      "\n",
      "Words in Line  106  :  ['MIZ.', '(Materialien', 'und', 'Informationen', 'zur', 'Zeit.', 'Politisches']\n",
      "\n",
      "Line  107  :  Journal der Konfessionslosesn und Atheisten. Hrsg. IBKA e.V.)\n",
      "\n",
      "Words in Line  107  :  ['Journal', 'der', 'Konfessionslosesn', 'und', 'Atheisten.', 'Hrsg.', 'IBKA', 'e.V.)']\n",
      "\n",
      "Line  108  :  MIZ-Vertrieb, Postfach 880, D-1000 Berlin 41. Germany.\n",
      "\n",
      "Words in Line  108  :  ['MIZ-Vertrieb,', 'Postfach', '880,', 'D-1000', 'Berlin', '41.', 'Germany.']\n",
      "\n",
      "Line  109  :  \n",
      "\n",
      "Words in Line  109  :  ['']\n",
      "\n",
      "Line  110  :  For atheist books, write to:\n",
      "\n",
      "Words in Line  110  :  ['For', 'atheist', 'books,', 'write', 'to:']\n",
      "\n",
      "Line  111  :  \n",
      "\n",
      "Words in Line  111  :  ['']\n",
      "\n",
      "Line  112  :  IBDK, Internationaler B\"ucherdienst der Konfessionslosen\n",
      "\n",
      "Words in Line  112  :  ['IBDK,', 'Internationaler', 'B\"ucherdienst', 'der', 'Konfessionslosen']\n",
      "\n",
      "Line  113  :  Postfach 3005, D-3000 Hannover 1. Germany.\n",
      "\n",
      "Words in Line  113  :  ['Postfach', '3005,', 'D-3000', 'Hannover', '1.', 'Germany.']\n",
      "\n",
      "Line  114  :  Telephone: 0511/211216\n",
      "\n",
      "Words in Line  114  :  ['Telephone:', '0511/211216']\n",
      "\n",
      "Line  115  :  \n",
      "\n",
      "Words in Line  115  :  ['']\n",
      "\n",
      "Line  116  :  \n",
      "\n",
      "Words in Line  116  :  ['']\n",
      "\n",
      "Line  117  :                                 Books -- Fiction\n",
      "\n",
      "Words in Line  117  :  ['Books', '--', 'Fiction']\n",
      "\n",
      "Line  118  :  \n",
      "\n",
      "Words in Line  118  :  ['']\n",
      "\n",
      "Line  119  :  THOMAS M. DISCH\n",
      "\n",
      "Words in Line  119  :  ['THOMAS', 'M.', 'DISCH']\n",
      "\n",
      "Line  120  :  \n",
      "\n",
      "Words in Line  120  :  ['']\n",
      "\n",
      "Line  121  :  \"The Santa Claus Compromise\"\n",
      "\n",
      "Words in Line  121  :  ['\"The', 'Santa', 'Claus', 'Compromise\"']\n",
      "\n",
      "Line  122  :  Short story.  The ultimate proof that Santa exists.  All characters and \n",
      "\n",
      "Words in Line  122  :  ['Short', 'story.', '', 'The', 'ultimate', 'proof', 'that', 'Santa', 'exists.', '', 'All', 'characters', 'and']\n",
      "\n",
      "Line  123  :  events are fictitious.  Any similarity to living or dead gods -- uh, well...\n",
      "\n",
      "Words in Line  123  :  ['events', 'are', 'fictitious.', '', 'Any', 'similarity', 'to', 'living', 'or', 'dead', 'gods', '--', 'uh,', 'well...']\n",
      "\n",
      "Line  124  :  \n",
      "\n",
      "Words in Line  124  :  ['']\n",
      "\n",
      "Line  125  :  WALTER M. MILLER, JR\n",
      "\n",
      "Words in Line  125  :  ['WALTER', 'M.', 'MILLER,', 'JR']\n",
      "\n",
      "Line  126  :  \n",
      "\n",
      "Words in Line  126  :  ['']\n",
      "\n",
      "Line  127  :  \"A Canticle for Leibowitz\"\n",
      "\n",
      "Words in Line  127  :  ['\"A', 'Canticle', 'for', 'Leibowitz\"']\n",
      "\n",
      "Line  128  :  One gem in this post atomic doomsday novel is the monks who spent their lives\n",
      "\n",
      "Words in Line  128  :  ['One', 'gem', 'in', 'this', 'post', 'atomic', 'doomsday', 'novel', 'is', 'the', 'monks', 'who', 'spent', 'their', 'lives']\n",
      "\n",
      "Line  129  :  copying blueprints from \"Saint Leibowitz\", filling the sheets of paper with\n",
      "\n",
      "Words in Line  129  :  ['copying', 'blueprints', 'from', '\"Saint', 'Leibowitz\",', 'filling', 'the', 'sheets', 'of', 'paper', 'with']\n",
      "\n",
      "Line  130  :  ink and leaving white lines and letters.\n",
      "\n",
      "Words in Line  130  :  ['ink', 'and', 'leaving', 'white', 'lines', 'and', 'letters.']\n",
      "\n",
      "Line  131  :  \n",
      "\n",
      "Words in Line  131  :  ['']\n",
      "\n",
      "Line  132  :  EDGAR PANGBORN\n",
      "\n",
      "Words in Line  132  :  ['EDGAR', 'PANGBORN']\n",
      "\n",
      "Line  133  :  \n",
      "\n",
      "Words in Line  133  :  ['']\n",
      "\n",
      "Line  134  :  \"Davy\"\n",
      "\n",
      "Words in Line  134  :  ['\"Davy\"']\n",
      "\n",
      "Line  135  :  Post atomic doomsday novel set in clerical states.  The church, for example,\n",
      "\n",
      "Words in Line  135  :  ['Post', 'atomic', 'doomsday', 'novel', 'set', 'in', 'clerical', 'states.', '', 'The', 'church,', 'for', 'example,']\n",
      "\n",
      "Line  136  :  forbids that anyone \"produce, describe or use any substance containing...\n",
      "\n",
      "Words in Line  136  :  ['forbids', 'that', 'anyone', '\"produce,', 'describe', 'or', 'use', 'any', 'substance', 'containing...']\n",
      "\n",
      "Line  137  :  atoms\". \n",
      "\n",
      "Words in Line  137  :  ['atoms\".']\n",
      "\n",
      "Line  138  :  \n",
      "\n",
      "Words in Line  138  :  ['']\n",
      "\n",
      "Line  139  :  PHILIP K. DICK\n",
      "\n",
      "Words in Line  139  :  ['PHILIP', 'K.', 'DICK']\n",
      "\n",
      "Line  140  :  \n",
      "\n",
      "Words in Line  140  :  ['']\n",
      "\n",
      "Line  141  :  Philip K. Dick Dick wrote many philosophical and thought-provoking short \n",
      "\n",
      "Words in Line  141  :  ['Philip', 'K.', 'Dick', 'Dick', 'wrote', 'many', 'philosophical', 'and', 'thought-provoking', 'short']\n",
      "\n",
      "Line  142  :  stories and novels.  His stories are bizarre at times, but very approachable.\n",
      "\n",
      "Words in Line  142  :  ['stories', 'and', 'novels.', '', 'His', 'stories', 'are', 'bizarre', 'at', 'times,', 'but', 'very', 'approachable.']\n",
      "\n",
      "Line  143  :  He wrote mainly SF, but he wrote about people, truth and religion rather than\n",
      "\n",
      "Words in Line  143  :  ['He', 'wrote', 'mainly', 'SF,', 'but', 'he', 'wrote', 'about', 'people,', 'truth', 'and', 'religion', 'rather', 'than']\n",
      "\n",
      "Line  144  :  technology.  Although he often believed that he had met some sort of God, he\n",
      "\n",
      "Words in Line  144  :  ['technology.', '', 'Although', 'he', 'often', 'believed', 'that', 'he', 'had', 'met', 'some', 'sort', 'of', 'God,', 'he']\n",
      "\n",
      "Line  145  :  remained sceptical.  Amongst his novels, the following are of some relevance:\n",
      "\n",
      "Words in Line  145  :  ['remained', 'sceptical.', '', 'Amongst', 'his', 'novels,', 'the', 'following', 'are', 'of', 'some', 'relevance:']\n",
      "\n",
      "Line  146  :  \n",
      "\n",
      "Words in Line  146  :  ['']\n",
      "\n",
      "Line  147  :  \"Galactic Pot-Healer\"\n",
      "\n",
      "Words in Line  147  :  ['\"Galactic', 'Pot-Healer\"']\n",
      "\n",
      "Line  148  :  A fallible alien deity summons a group of Earth craftsmen and women to a\n",
      "\n",
      "Words in Line  148  :  ['A', 'fallible', 'alien', 'deity', 'summons', 'a', 'group', 'of', 'Earth', 'craftsmen', 'and', 'women', 'to', 'a']\n",
      "\n",
      "Line  149  :  remote planet to raise a giant cathedral from beneath the oceans.  When the\n",
      "\n",
      "Words in Line  149  :  ['remote', 'planet', 'to', 'raise', 'a', 'giant', 'cathedral', 'from', 'beneath', 'the', 'oceans.', '', 'When', 'the']\n",
      "\n",
      "Line  150  :  deity begins to demand faith from the earthers, pot-healer Joe Fernwright is\n",
      "\n",
      "Words in Line  150  :  ['deity', 'begins', 'to', 'demand', 'faith', 'from', 'the', 'earthers,', 'pot-healer', 'Joe', 'Fernwright', 'is']\n",
      "\n",
      "Line  151  :  unable to comply.  A polished, ironic and amusing novel.\n",
      "\n",
      "Words in Line  151  :  ['unable', 'to', 'comply.', '', 'A', 'polished,', 'ironic', 'and', 'amusing', 'novel.']\n",
      "\n",
      "Line  152  :  \n",
      "\n",
      "Words in Line  152  :  ['']\n",
      "\n",
      "Line  153  :  \"A Maze of Death\"\n",
      "\n",
      "Words in Line  153  :  ['\"A', 'Maze', 'of', 'Death\"']\n",
      "\n",
      "Line  154  :  Noteworthy for its description of a technology-based religion.\n",
      "\n",
      "Words in Line  154  :  ['Noteworthy', 'for', 'its', 'description', 'of', 'a', 'technology-based', 'religion.']\n",
      "\n",
      "Line  155  :  \n",
      "\n",
      "Words in Line  155  :  ['']\n",
      "\n",
      "Line  156  :  \"VALIS\"\n",
      "\n",
      "Words in Line  156  :  ['\"VALIS\"']\n",
      "\n",
      "Line  157  :  The schizophrenic hero searches for the hidden mysteries of Gnostic\n",
      "\n",
      "Words in Line  157  :  ['The', 'schizophrenic', 'hero', 'searches', 'for', 'the', 'hidden', 'mysteries', 'of', 'Gnostic']\n",
      "\n",
      "Line  158  :  Christianity after reality is fired into his brain by a pink laser beam of\n",
      "\n",
      "Words in Line  158  :  ['Christianity', 'after', 'reality', 'is', 'fired', 'into', 'his', 'brain', 'by', 'a', 'pink', 'laser', 'beam', 'of']\n",
      "\n",
      "Line  159  :  unknown but possibly divine origin.  He is accompanied by his dogmatic and\n",
      "\n",
      "Words in Line  159  :  ['unknown', 'but', 'possibly', 'divine', 'origin.', '', 'He', 'is', 'accompanied', 'by', 'his', 'dogmatic', 'and']\n",
      "\n",
      "Line  160  :  dismissively atheist friend and assorted other odd characters.\n",
      "\n",
      "Words in Line  160  :  ['dismissively', 'atheist', 'friend', 'and', 'assorted', 'other', 'odd', 'characters.']\n",
      "\n",
      "Line  161  :  \n",
      "\n",
      "Words in Line  161  :  ['']\n",
      "\n",
      "Line  162  :  \"The Divine Invasion\"\n",
      "\n",
      "Words in Line  162  :  ['\"The', 'Divine', 'Invasion\"']\n",
      "\n",
      "Line  163  :  God invades Earth by making a young woman pregnant as she returns from\n",
      "\n",
      "Words in Line  163  :  ['God', 'invades', 'Earth', 'by', 'making', 'a', 'young', 'woman', 'pregnant', 'as', 'she', 'returns', 'from']\n",
      "\n",
      "Line  164  :  another star system.  Unfortunately she is terminally ill, and must be\n",
      "\n",
      "Words in Line  164  :  ['another', 'star', 'system.', '', 'Unfortunately', 'she', 'is', 'terminally', 'ill,', 'and', 'must', 'be']\n",
      "\n",
      "Line  165  :  assisted by a dead man whose brain is wired to 24-hour easy listening music.\n",
      "\n",
      "Words in Line  165  :  ['assisted', 'by', 'a', 'dead', 'man', 'whose', 'brain', 'is', 'wired', 'to', '24-hour', 'easy', 'listening', 'music.']\n",
      "\n",
      "Line  166  :  \n",
      "\n",
      "Words in Line  166  :  ['']\n",
      "\n",
      "Line  167  :  MARGARET ATWOOD\n",
      "\n",
      "Words in Line  167  :  ['MARGARET', 'ATWOOD']\n",
      "\n",
      "Line  168  :  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Line  168  :  ['']\n",
      "\n",
      "Line  169  :  \"The Handmaid's Tale\"\n",
      "\n",
      "Words in Line  169  :  ['\"The', \"Handmaid's\", 'Tale\"']\n",
      "\n",
      "Line  170  :  A story based on the premise that the US Congress is mysteriously\n",
      "\n",
      "Words in Line  170  :  ['A', 'story', 'based', 'on', 'the', 'premise', 'that', 'the', 'US', 'Congress', 'is', 'mysteriously']\n",
      "\n",
      "Line  171  :  assassinated, and fundamentalists quickly take charge of the nation to set it\n",
      "\n",
      "Words in Line  171  :  ['assassinated,', 'and', 'fundamentalists', 'quickly', 'take', 'charge', 'of', 'the', 'nation', 'to', 'set', 'it']\n",
      "\n",
      "Line  172  :  \"right\" again.  The book is the diary of a woman's life as she tries to live\n",
      "\n",
      "Words in Line  172  :  ['\"right\"', 'again.', '', 'The', 'book', 'is', 'the', 'diary', 'of', 'a', \"woman's\", 'life', 'as', 'she', 'tries', 'to', 'live']\n",
      "\n",
      "Line  173  :  under the new Christian theocracy.  Women's right to own property is revoked,\n",
      "\n",
      "Words in Line  173  :  ['under', 'the', 'new', 'Christian', 'theocracy.', '', \"Women's\", 'right', 'to', 'own', 'property', 'is', 'revoked,']\n",
      "\n",
      "Line  174  :  and their bank accounts are closed; sinful luxuries are outlawed, and the\n",
      "\n",
      "Words in Line  174  :  ['and', 'their', 'bank', 'accounts', 'are', 'closed;', 'sinful', 'luxuries', 'are', 'outlawed,', 'and', 'the']\n",
      "\n",
      "Line  175  :  radio is only used for readings from the Bible.  Crimes are punished\n",
      "\n",
      "Words in Line  175  :  ['radio', 'is', 'only', 'used', 'for', 'readings', 'from', 'the', 'Bible.', '', 'Crimes', 'are', 'punished']\n",
      "\n",
      "Line  176  :  retroactively: doctors who performed legal abortions in the \"old world\" are\n",
      "\n",
      "Words in Line  176  :  ['retroactively:', 'doctors', 'who', 'performed', 'legal', 'abortions', 'in', 'the', '\"old', 'world\"', 'are']\n",
      "\n",
      "Line  177  :  hunted down and hanged.  Atwood's writing style is difficult to get used to\n",
      "\n",
      "Words in Line  177  :  ['hunted', 'down', 'and', 'hanged.', '', \"Atwood's\", 'writing', 'style', 'is', 'difficult', 'to', 'get', 'used', 'to']\n",
      "\n",
      "Line  178  :  at first, but the tale grows more and more chilling as it goes on.\n",
      "\n",
      "Words in Line  178  :  ['at', 'first,', 'but', 'the', 'tale', 'grows', 'more', 'and', 'more', 'chilling', 'as', 'it', 'goes', 'on.']\n",
      "\n",
      "Line  179  :  \n",
      "\n",
      "Words in Line  179  :  ['']\n",
      "\n",
      "Line  180  :  VARIOUS AUTHORS\n",
      "\n",
      "Words in Line  180  :  ['VARIOUS', 'AUTHORS']\n",
      "\n",
      "Line  181  :  \n",
      "\n",
      "Words in Line  181  :  ['']\n",
      "\n",
      "Line  182  :  \"The Bible\"\n",
      "\n",
      "Words in Line  182  :  ['\"The', 'Bible\"']\n",
      "\n",
      "Line  183  :  This somewhat dull and rambling work has often been criticized.  However, it\n",
      "\n",
      "Words in Line  183  :  ['This', 'somewhat', 'dull', 'and', 'rambling', 'work', 'has', 'often', 'been', 'criticized.', '', 'However,', 'it']\n",
      "\n",
      "Line  184  :  is probably worth reading, if only so that you'll know what all the fuss is\n",
      "\n",
      "Words in Line  184  :  ['is', 'probably', 'worth', 'reading,', 'if', 'only', 'so', 'that', \"you'll\", 'know', 'what', 'all', 'the', 'fuss', 'is']\n",
      "\n",
      "Line  185  :  about.  It exists in many different versions, so make sure you get the one\n",
      "\n",
      "Words in Line  185  :  ['about.', '', 'It', 'exists', 'in', 'many', 'different', 'versions,', 'so', 'make', 'sure', 'you', 'get', 'the', 'one']\n",
      "\n",
      "Line  186  :  true version.\n",
      "\n",
      "Words in Line  186  :  ['true', 'version.']\n",
      "\n",
      "Line  187  :  \n",
      "\n",
      "Words in Line  187  :  ['']\n",
      "\n",
      "Line  188  :                               Books -- Non-fiction\n",
      "\n",
      "Words in Line  188  :  ['Books', '--', 'Non-fiction']\n",
      "\n",
      "Line  189  :  \n",
      "\n",
      "Words in Line  189  :  ['']\n",
      "\n",
      "Line  190  :  PETER DE ROSA\n",
      "\n",
      "Words in Line  190  :  ['PETER', 'DE', 'ROSA']\n",
      "\n",
      "Line  191  :  \n",
      "\n",
      "Words in Line  191  :  ['']\n",
      "\n",
      "Line  192  :  \"Vicars of Christ\", Bantam Press, 1988\n",
      "\n",
      "Words in Line  192  :  ['\"Vicars', 'of', 'Christ\",', 'Bantam', 'Press,', '1988']\n",
      "\n",
      "Line  193  :  Although de Rosa seems to be Christian or even Catholic this is a very\n",
      "\n",
      "Words in Line  193  :  ['Although', 'de', 'Rosa', 'seems', 'to', 'be', 'Christian', 'or', 'even', 'Catholic', 'this', 'is', 'a', 'very']\n",
      "\n",
      "Line  194  :  enlighting history of papal immoralities, adulteries, fallacies etc.\n",
      "\n",
      "Words in Line  194  :  ['enlighting', 'history', 'of', 'papal', 'immoralities,', 'adulteries,', 'fallacies', 'etc.']\n",
      "\n",
      "Line  195  :  (German translation: \"Gottes erste Diener. Die dunkle Seite des Papsttums\",\n",
      "\n",
      "Words in Line  195  :  ['(German', 'translation:', '\"Gottes', 'erste', 'Diener.', 'Die', 'dunkle', 'Seite', 'des', 'Papsttums\",']\n",
      "\n",
      "Line  196  :  Droemer-Knaur, 1989)\n",
      "\n",
      "Words in Line  196  :  ['Droemer-Knaur,', '1989)']\n",
      "\n",
      "Line  197  :  \n",
      "\n",
      "Words in Line  197  :  ['']\n",
      "\n",
      "Line  198  :  MICHAEL MARTIN\n",
      "\n",
      "Words in Line  198  :  ['MICHAEL', 'MARTIN']\n",
      "\n",
      "Line  199  :  \n",
      "\n",
      "Words in Line  199  :  ['']\n",
      "\n",
      "Line  200  :  \"Atheism: A Philosophical Justification\", Temple University Press,\n",
      "\n",
      "Words in Line  200  :  ['\"Atheism:', 'A', 'Philosophical', 'Justification\",', 'Temple', 'University', 'Press,']\n",
      "\n",
      "Line  201  :   Philadelphia, USA.\n",
      "\n",
      "Words in Line  201  :  ['Philadelphia,', 'USA.']\n",
      "\n",
      "Line  202  :  A detailed and scholarly justification of atheism.  Contains an outstanding\n",
      "\n",
      "Words in Line  202  :  ['A', 'detailed', 'and', 'scholarly', 'justification', 'of', 'atheism.', '', 'Contains', 'an', 'outstanding']\n",
      "\n",
      "Line  203  :  appendix defining terminology and usage in this (necessarily) tendentious\n",
      "\n",
      "Words in Line  203  :  ['appendix', 'defining', 'terminology', 'and', 'usage', 'in', 'this', '(necessarily)', 'tendentious']\n",
      "\n",
      "Line  204  :  area.  Argues both for \"negative atheism\" (i.e. the \"non-belief in the\n",
      "\n",
      "Words in Line  204  :  ['area.', '', 'Argues', 'both', 'for', '\"negative', 'atheism\"', '(i.e.', 'the', '\"non-belief', 'in', 'the']\n",
      "\n",
      "Line  205  :  existence of god(s)\") and also for \"positive atheism\" (\"the belief in the\n",
      "\n",
      "Words in Line  205  :  ['existence', 'of', 'god(s)\")', 'and', 'also', 'for', '\"positive', 'atheism\"', '(\"the', 'belief', 'in', 'the']\n",
      "\n",
      "Line  206  :  non-existence of god(s)\").  Includes great refutations of the most\n",
      "\n",
      "Words in Line  206  :  ['non-existence', 'of', 'god(s)\").', '', 'Includes', 'great', 'refutations', 'of', 'the', 'most']\n",
      "\n",
      "Line  207  :  challenging arguments for god; particular attention is paid to refuting\n",
      "\n",
      "Words in Line  207  :  ['challenging', 'arguments', 'for', 'god;', 'particular', 'attention', 'is', 'paid', 'to', 'refuting']\n",
      "\n",
      "Line  208  :  contempory theists such as Platinga and Swinburne.\n",
      "\n",
      "Words in Line  208  :  ['contempory', 'theists', 'such', 'as', 'Platinga', 'and', 'Swinburne.']\n",
      "\n",
      "Line  209  :  541 pages. ISBN 0-87722-642-3 (hardcover; paperback also available)\n",
      "\n",
      "Words in Line  209  :  ['541', 'pages.', 'ISBN', '0-87722-642-3', '(hardcover;', 'paperback', 'also', 'available)']\n",
      "\n",
      "Line  210  :  \n",
      "\n",
      "Words in Line  210  :  ['']\n",
      "\n",
      "Line  211  :  \"The Case Against Christianity\", Temple University Press\n",
      "\n",
      "Words in Line  211  :  ['\"The', 'Case', 'Against', 'Christianity\",', 'Temple', 'University', 'Press']\n",
      "\n",
      "Line  212  :  A comprehensive critique of Christianity, in which he considers\n",
      "\n",
      "Words in Line  212  :  ['A', 'comprehensive', 'critique', 'of', 'Christianity,', 'in', 'which', 'he', 'considers']\n",
      "\n",
      "Line  213  :  the best contemporary defences of Christianity and (ultimately)\n",
      "\n",
      "Words in Line  213  :  ['the', 'best', 'contemporary', 'defences', 'of', 'Christianity', 'and', '(ultimately)']\n",
      "\n",
      "Line  214  :  demonstrates that they are unsupportable and/or incoherent.\n",
      "\n",
      "Words in Line  214  :  ['demonstrates', 'that', 'they', 'are', 'unsupportable', 'and/or', 'incoherent.']\n",
      "\n",
      "Line  215  :  273 pages. ISBN 0-87722-767-5\n",
      "\n",
      "Words in Line  215  :  ['273', 'pages.', 'ISBN', '0-87722-767-5']\n",
      "\n",
      "Line  216  :  \n",
      "\n",
      "Words in Line  216  :  ['']\n",
      "\n",
      "Line  217  :  JAMES TURNER\n",
      "\n",
      "Words in Line  217  :  ['JAMES', 'TURNER']\n",
      "\n",
      "Line  218  :  \n",
      "\n",
      "Words in Line  218  :  ['']\n",
      "\n",
      "Line  219  :  \"Without God, Without Creed\", The Johns Hopkins University Press, Baltimore,\n",
      "\n",
      "Words in Line  219  :  ['\"Without', 'God,', 'Without', 'Creed\",', 'The', 'Johns', 'Hopkins', 'University', 'Press,', 'Baltimore,']\n",
      "\n",
      "Line  220  :   MD, USA\n",
      "\n",
      "Words in Line  220  :  ['MD,', 'USA']\n",
      "\n",
      "Line  221  :  Subtitled \"The Origins of Unbelief in America\".  Examines the way in which\n",
      "\n",
      "Words in Line  221  :  ['Subtitled', '\"The', 'Origins', 'of', 'Unbelief', 'in', 'America\".', '', 'Examines', 'the', 'way', 'in', 'which']\n",
      "\n",
      "Line  222  :  unbelief (whether agnostic or atheistic)  became a mainstream alternative\n",
      "\n",
      "Words in Line  222  :  ['unbelief', '(whether', 'agnostic', 'or', 'atheistic)', '', 'became', 'a', 'mainstream', 'alternative']\n",
      "\n",
      "Line  223  :  world-view.  Focusses on the period 1770-1900, and while considering France\n",
      "\n",
      "Words in Line  223  :  ['world-view.', '', 'Focusses', 'on', 'the', 'period', '1770-1900,', 'and', 'while', 'considering', 'France']\n",
      "\n",
      "Line  224  :  and Britain the emphasis is on American, and particularly New England\n",
      "\n",
      "Words in Line  224  :  ['and', 'Britain', 'the', 'emphasis', 'is', 'on', 'American,', 'and', 'particularly', 'New', 'England']\n",
      "\n",
      "Line  225  :  developments.  \"Neither a religious history of secularization or atheism,\n",
      "\n",
      "Words in Line  225  :  ['developments.', '', '\"Neither', 'a', 'religious', 'history', 'of', 'secularization', 'or', 'atheism,']\n",
      "\n",
      "Line  226  :  Without God, Without Creed is, rather, the intellectual history of the fate\n",
      "\n",
      "Words in Line  226  :  ['Without', 'God,', 'Without', 'Creed', 'is,', 'rather,', 'the', 'intellectual', 'history', 'of', 'the', 'fate']\n",
      "\n",
      "Line  227  :  of a single idea, the belief that God exists.\" \n",
      "\n",
      "Words in Line  227  :  ['of', 'a', 'single', 'idea,', 'the', 'belief', 'that', 'God', 'exists.\"']\n",
      "\n",
      "Line  228  :  316 pages. ISBN (hardcover) 0-8018-2494-X (paper) 0-8018-3407-4\n",
      "\n",
      "Words in Line  228  :  ['316', 'pages.', 'ISBN', '(hardcover)', '0-8018-2494-X', '(paper)', '0-8018-3407-4']\n",
      "\n",
      "Line  229  :  \n",
      "\n",
      "Words in Line  229  :  ['']\n",
      "\n",
      "Line  230  :  GEORGE SELDES (Editor)\n",
      "\n",
      "Words in Line  230  :  ['GEORGE', 'SELDES', '(Editor)']\n",
      "\n",
      "Line  231  :  \n",
      "\n",
      "Words in Line  231  :  ['']\n",
      "\n",
      "Line  232  :  \"The great thoughts\", Ballantine Books, New York, USA\n",
      "\n",
      "Words in Line  232  :  ['\"The', 'great', 'thoughts\",', 'Ballantine', 'Books,', 'New', 'York,', 'USA']\n",
      "\n",
      "Line  233  :  A \"dictionary of quotations\" of a different kind, concentrating on statements\n",
      "\n",
      "Words in Line  233  :  ['A', '\"dictionary', 'of', 'quotations\"', 'of', 'a', 'different', 'kind,', 'concentrating', 'on', 'statements']\n",
      "\n",
      "Line  234  :  and writings which, explicitly or implicitly, present the person's philosophy\n",
      "\n",
      "Words in Line  234  :  ['and', 'writings', 'which,', 'explicitly', 'or', 'implicitly,', 'present', 'the', \"person's\", 'philosophy']\n",
      "\n",
      "Line  235  :  and world-view.  Includes obscure (and often suppressed) opinions from many\n",
      "\n",
      "Words in Line  235  :  ['and', 'world-view.', '', 'Includes', 'obscure', '(and', 'often', 'suppressed)', 'opinions', 'from', 'many']\n",
      "\n",
      "Line  236  :  people.  For some popular observations, traces the way in which various\n",
      "\n",
      "Words in Line  236  :  ['people.', '', 'For', 'some', 'popular', 'observations,', 'traces', 'the', 'way', 'in', 'which', 'various']\n",
      "\n",
      "Line  237  :  people expressed and twisted the idea over the centuries.  Quite a number of\n",
      "\n",
      "Words in Line  237  :  ['people', 'expressed', 'and', 'twisted', 'the', 'idea', 'over', 'the', 'centuries.', '', 'Quite', 'a', 'number', 'of']\n",
      "\n",
      "Line  238  :  the quotations are derived from Cardiff's \"What Great Men Think of Religion\"\n",
      "\n",
      "Words in Line  238  :  ['the', 'quotations', 'are', 'derived', 'from', \"Cardiff's\", '\"What', 'Great', 'Men', 'Think', 'of', 'Religion\"']\n",
      "\n",
      "Line  239  :  and Noyes' \"Views of Religion\".\n",
      "\n",
      "Words in Line  239  :  ['and', \"Noyes'\", '\"Views', 'of', 'Religion\".']\n",
      "\n",
      "Line  240  :  490 pages. ISBN (paper) 0-345-29887-X.\n",
      "\n",
      "Words in Line  240  :  ['490', 'pages.', 'ISBN', '(paper)', '0-345-29887-X.']\n",
      "\n",
      "Line  241  :  \n",
      "\n",
      "Words in Line  241  :  ['']\n",
      "\n",
      "Line  242  :  RICHARD SWINBURNE\n",
      "\n",
      "Words in Line  242  :  ['RICHARD', 'SWINBURNE']\n",
      "\n",
      "Line  243  :  \n",
      "\n",
      "Words in Line  243  :  ['']\n",
      "\n",
      "Line  244  :  \"The Existence of God (Revised Edition)\", Clarendon Paperbacks, Oxford\n",
      "\n",
      "Words in Line  244  :  ['\"The', 'Existence', 'of', 'God', '(Revised', 'Edition)\",', 'Clarendon', 'Paperbacks,', 'Oxford']\n",
      "\n",
      "Line  245  :  This book is the second volume in a trilogy that began with \"The Coherence of\n",
      "\n",
      "Words in Line  245  :  ['This', 'book', 'is', 'the', 'second', 'volume', 'in', 'a', 'trilogy', 'that', 'began', 'with', '\"The', 'Coherence', 'of']\n",
      "\n",
      "Line  246  :  Theism\" (1977) and was concluded with \"Faith and Reason\" (1981).  In this\n",
      "\n",
      "Words in Line  246  :  ['Theism\"', '(1977)', 'and', 'was', 'concluded', 'with', '\"Faith', 'and', 'Reason\"', '(1981).', '', 'In', 'this']\n",
      "\n",
      "Line  247  :  work, Swinburne attempts to construct a series of inductive arguments for the\n",
      "\n",
      "Words in Line  247  :  ['work,', 'Swinburne', 'attempts', 'to', 'construct', 'a', 'series', 'of', 'inductive', 'arguments', 'for', 'the']\n",
      "\n",
      "Line  248  :  existence of God.  His arguments, which are somewhat tendentious and rely\n",
      "\n",
      "Words in Line  248  :  ['existence', 'of', 'God.', '', 'His', 'arguments,', 'which', 'are', 'somewhat', 'tendentious', 'and', 'rely']\n",
      "\n",
      "Line  249  :  upon the imputation of late 20th century western Christian values and\n",
      "\n",
      "Words in Line  249  :  ['upon', 'the', 'imputation', 'of', 'late', '20th', 'century', 'western', 'Christian', 'values', 'and']\n",
      "\n",
      "Line  250  :  aesthetics to a God which is supposedly as simple as can be conceived, were\n",
      "\n",
      "Words in Line  250  :  ['aesthetics', 'to', 'a', 'God', 'which', 'is', 'supposedly', 'as', 'simple', 'as', 'can', 'be', 'conceived,', 'were']\n",
      "\n",
      "Line  251  :  decisively rejected in Mackie's \"The Miracle of Theism\".  In the revised\n",
      "\n",
      "Words in Line  251  :  ['decisively', 'rejected', 'in', \"Mackie's\", '\"The', 'Miracle', 'of', 'Theism\".', '', 'In', 'the', 'revised']\n",
      "\n",
      "Line  252  :  edition of \"The Existence of God\", Swinburne includes an Appendix in which he\n",
      "\n",
      "Words in Line  252  :  ['edition', 'of', '\"The', 'Existence', 'of', 'God\",', 'Swinburne', 'includes', 'an', 'Appendix', 'in', 'which', 'he']\n",
      "\n",
      "Line  253  :  makes a somewhat incoherent attempt to rebut Mackie.\n",
      "\n",
      "Words in Line  253  :  ['makes', 'a', 'somewhat', 'incoherent', 'attempt', 'to', 'rebut', 'Mackie.']\n",
      "\n",
      "Line  254  :  \n",
      "\n",
      "Words in Line  254  :  ['']\n",
      "\n",
      "Line  255  :  J. L. MACKIE\n",
      "\n",
      "Words in Line  255  :  ['J.', 'L.', 'MACKIE']\n",
      "\n",
      "Line  256  :  \n",
      "\n",
      "Words in Line  256  :  ['']\n",
      "\n",
      "Line  257  :  \"The Miracle of Theism\", Oxford\n",
      "\n",
      "Words in Line  257  :  ['\"The', 'Miracle', 'of', 'Theism\",', 'Oxford']\n",
      "\n",
      "Line  258  :  This (posthumous) volume contains a comprehensive review of the principal\n",
      "\n",
      "Words in Line  258  :  ['This', '(posthumous)', 'volume', 'contains', 'a', 'comprehensive', 'review', 'of', 'the', 'principal']\n",
      "\n",
      "Line  259  :  arguments for and against the existence of God.  It ranges from the classical\n",
      "\n",
      "Words in Line  259  :  ['arguments', 'for', 'and', 'against', 'the', 'existence', 'of', 'God.', '', 'It', 'ranges', 'from', 'the', 'classical']\n",
      "\n",
      "Line  260  :  philosophical positions of Descartes, Anselm, Berkeley, Hume et al, through\n",
      "\n",
      "Words in Line  260  :  ['philosophical', 'positions', 'of', 'Descartes,', 'Anselm,', 'Berkeley,', 'Hume', 'et', 'al,', 'through']\n",
      "\n",
      "Line  261  :  the moral arguments of Newman, Kant and Sidgwick, to the recent restatements\n",
      "\n",
      "Words in Line  261  :  ['the', 'moral', 'arguments', 'of', 'Newman,', 'Kant', 'and', 'Sidgwick,', 'to', 'the', 'recent', 'restatements']\n",
      "\n",
      "Line  262  :  of the classical theses by Plantinga and Swinburne.  It also addresses those\n",
      "\n",
      "Words in Line  262  :  ['of', 'the', 'classical', 'theses', 'by', 'Plantinga', 'and', 'Swinburne.', '', 'It', 'also', 'addresses', 'those']\n",
      "\n",
      "Line  263  :  positions which push the concept of God beyond the realm of the rational,\n",
      "\n",
      "Words in Line  263  :  ['positions', 'which', 'push', 'the', 'concept', 'of', 'God', 'beyond', 'the', 'realm', 'of', 'the', 'rational,']\n",
      "\n",
      "Line  264  :  such as those of Kierkegaard, Kung and Philips, as well as \"replacements for\n",
      "\n",
      "Words in Line  264  :  ['such', 'as', 'those', 'of', 'Kierkegaard,', 'Kung', 'and', 'Philips,', 'as', 'well', 'as', '\"replacements', 'for']\n",
      "\n",
      "Line  265  :  God\" such as Lelie's axiarchism.  The book is a delight to read - less\n",
      "\n",
      "Words in Line  265  :  ['God\"', 'such', 'as', \"Lelie's\", 'axiarchism.', '', 'The', 'book', 'is', 'a', 'delight', 'to', 'read', '-', 'less']\n",
      "\n",
      "Line  266  :  formalistic and better written than Martin's works, and refreshingly direct\n",
      "\n",
      "Words in Line  266  :  ['formalistic', 'and', 'better', 'written', 'than', \"Martin's\", 'works,', 'and', 'refreshingly', 'direct']\n",
      "\n",
      "Line  267  :  when compared with the hand-waving of Swinburne.\n",
      "\n",
      "Words in Line  267  :  ['when', 'compared', 'with', 'the', 'hand-waving', 'of', 'Swinburne.']\n",
      "\n",
      "Line  268  :  \n",
      "\n",
      "Words in Line  268  :  ['']\n",
      "\n",
      "Line  269  :  JAMES A. HAUGHT\n",
      "\n",
      "Words in Line  269  :  ['JAMES', 'A.', 'HAUGHT']\n",
      "\n",
      "Line  270  :  \n",
      "\n",
      "Words in Line  270  :  ['']\n",
      "\n",
      "Line  271  :  \"Holy Horrors: An Illustrated History of Religious Murder and Madness\",\n",
      "\n",
      "Words in Line  271  :  ['\"Holy', 'Horrors:', 'An', 'Illustrated', 'History', 'of', 'Religious', 'Murder', 'and', 'Madness\",']\n",
      "\n",
      "Line  272  :   Prometheus Books\n",
      "\n",
      "Words in Line  272  :  ['Prometheus', 'Books']\n",
      "\n",
      "Line  273  :  Looks at religious persecution from ancient times to the present day -- and\n",
      "\n",
      "Words in Line  273  :  ['Looks', 'at', 'religious', 'persecution', 'from', 'ancient', 'times', 'to', 'the', 'present', 'day', '--', 'and']\n",
      "\n",
      "Line  274  :  not only by Christians.\n",
      "\n",
      "Words in Line  274  :  ['not', 'only', 'by', 'Christians.']\n",
      "\n",
      "Line  275  :  Library of Congress Catalog Card Number 89-64079. 1990.\n",
      "\n",
      "Words in Line  275  :  ['Library', 'of', 'Congress', 'Catalog', 'Card', 'Number', '89-64079.', '1990.']\n",
      "\n",
      "Line  276  :  \n",
      "\n",
      "Words in Line  276  :  ['']\n",
      "\n",
      "Line  277  :  NORM R. ALLEN, JR.\n",
      "\n",
      "Words in Line  277  :  ['NORM', 'R.', 'ALLEN,', 'JR.']\n",
      "\n",
      "Line  278  :  \n",
      "\n",
      "Words in Line  278  :  ['']\n",
      "\n",
      "Line  279  :  \"African American Humanism: an Anthology\"\n",
      "\n",
      "Words in Line  279  :  ['\"African', 'American', 'Humanism:', 'an', 'Anthology\"']\n",
      "\n",
      "Line  280  :  See the listing for African Americans for Humanism above.\n",
      "\n",
      "Words in Line  280  :  ['See', 'the', 'listing', 'for', 'African', 'Americans', 'for', 'Humanism', 'above.']\n",
      "\n",
      "Line  281  :  \n",
      "\n",
      "Words in Line  281  :  ['']\n",
      "\n",
      "Line  282  :  GORDON STEIN\n",
      "\n",
      "Words in Line  282  :  ['GORDON', 'STEIN']\n",
      "\n",
      "Line  283  :  \n",
      "\n",
      "Words in Line  283  :  ['']\n",
      "\n",
      "Line  284  :  \"An Anthology of Atheism and Rationalism\", Prometheus Books\n",
      "\n",
      "Words in Line  284  :  ['\"An', 'Anthology', 'of', 'Atheism', 'and', 'Rationalism\",', 'Prometheus', 'Books']\n",
      "\n",
      "Line  285  :  An anthology covering a wide range of subjects, including 'The Devil, Evil\n",
      "\n",
      "Words in Line  285  :  ['An', 'anthology', 'covering', 'a', 'wide', 'range', 'of', 'subjects,', 'including', \"'The\", 'Devil,', 'Evil']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line  286  :  and Morality' and 'The History of Freethought'.  Comprehensive bibliography.\n",
      "\n",
      "Words in Line  286  :  ['and', \"Morality'\", 'and', \"'The\", 'History', 'of', \"Freethought'.\", '', 'Comprehensive', 'bibliography.']\n",
      "\n",
      "Line  287  :  \n",
      "\n",
      "Words in Line  287  :  ['']\n",
      "\n",
      "Line  288  :  EDMUND D. COHEN\n",
      "\n",
      "Words in Line  288  :  ['EDMUND', 'D.', 'COHEN']\n",
      "\n",
      "Line  289  :  \n",
      "\n",
      "Words in Line  289  :  ['']\n",
      "\n",
      "Line  290  :  \"The Mind of The Bible-Believer\", Prometheus Books\n",
      "\n",
      "Words in Line  290  :  ['\"The', 'Mind', 'of', 'The', 'Bible-Believer\",', 'Prometheus', 'Books']\n",
      "\n",
      "Line  291  :  A study of why people become Christian fundamentalists, and what effect it\n",
      "\n",
      "Words in Line  291  :  ['A', 'study', 'of', 'why', 'people', 'become', 'Christian', 'fundamentalists,', 'and', 'what', 'effect', 'it']\n",
      "\n",
      "Line  292  :  has on them.\n",
      "\n",
      "Words in Line  292  :  ['has', 'on', 'them.']\n",
      "\n",
      "Line  293  :  \n",
      "\n",
      "Words in Line  293  :  ['']\n",
      "\n",
      "Line  294  :                                  Net Resources\n",
      "\n",
      "Words in Line  294  :  ['Net', 'Resources']\n",
      "\n",
      "Line  295  :  \n",
      "\n",
      "Words in Line  295  :  ['']\n",
      "\n",
      "Line  296  :  There's a small mail-based archive server at mantis.co.uk which carries\n",
      "\n",
      "Words in Line  296  :  [\"There's\", 'a', 'small', 'mail-based', 'archive', 'server', 'at', 'mantis.co.uk', 'which', 'carries']\n",
      "\n",
      "Line  297  :  archives of old alt.atheism.moderated articles and assorted other files.  For\n",
      "\n",
      "Words in Line  297  :  ['archives', 'of', 'old', 'alt.atheism.moderated', 'articles', 'and', 'assorted', 'other', 'files.', '', 'For']\n",
      "\n",
      "Line  298  :  more information, send mail to archive-server@mantis.co.uk saying\n",
      "\n",
      "Words in Line  298  :  ['more', 'information,', 'send', 'mail', 'to', 'archive-server@mantis.co.uk', 'saying']\n",
      "\n",
      "Line  299  :  \n",
      "\n",
      "Words in Line  299  :  ['']\n",
      "\n",
      "Line  300  :     help\n",
      "\n",
      "Words in Line  300  :  ['help']\n",
      "\n",
      "Line  301  :     send atheism/index\n",
      "\n",
      "Words in Line  301  :  ['send', 'atheism/index']\n",
      "\n",
      "Line  302  :  \n",
      "\n",
      "Words in Line  302  :  ['']\n",
      "\n",
      "Line  303  :  and it will mail back a reply.\n",
      "\n",
      "Words in Line  303  :  ['and', 'it', 'will', 'mail', 'back', 'a', 'reply.']\n",
      "\n",
      "Line  304  :  \n",
      "\n",
      "Words in Line  304  :  ['']\n",
      "\n",
      "Line  305  :  \n",
      "\n",
      "Words in Line  305  :  ['']\n",
      "\n",
      "Line  306  :  mathew\n",
      "\n",
      "Words in Line  306  :  ['mathew']\n",
      "\n",
      "Line  307  :  ÿ\n",
      "\n",
      "Words in Line  307  :  ['ÿ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we are just trying to open a single file and see how the things are working\n",
    "# YOU CAN SKIP THIS IF YOU WANT\n",
    "# As all this code explaination is given in the next cell\n",
    "\n",
    "i,j = 1,1\n",
    "\n",
    "for category in categories:\n",
    "    if(i != 1):\n",
    "        break\n",
    "    path_category = path_20_newsgrp + '/' + category\n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        if(j != 1):\n",
    "            break\n",
    "        file = open(path_category + '/' + file_id,'r',encoding=\"ISO-8859-1\")\n",
    "        k = 1\n",
    "        for line in file:\n",
    "            string = str(line)\n",
    "            # print('Line ',k,' : ',string)\n",
    "            words = string.strip().split(' ')\n",
    "            # print('Words in Line ',k,' : ',words)\n",
    "            # print()\n",
    "            k += 1\n",
    "        j += 1\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc', 'total_data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {}\n",
    "# Count is our Vocabulary\n",
    "# The top level keys in this vocabulary are all the 20 possible class_types and the 'total_data' which represents the total number of files\n",
    "# For each of the class_type(KEY) we are having another dictionary (as the VALUE) which is given as:\n",
    "# Word occuring within each class(AS KEY) and it's frequency(AS VALUE)\n",
    "# Each class_type is also having a key 'total_count' which represents the total number of files corresponding to that particular class\n",
    "\n",
    "\n",
    "total_files = 0   # For count of total files\n",
    "total = 0         # For count of files for a particular class\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    # Iterating over all the 20 categories/class_types\n",
    "    \n",
    "    d = {}                                              # This is the dictionary which will be added to each category as value\n",
    "    path_category = path_20_newsgrp + '/' + category    # This is the path of each category's folder\n",
    "    files = os.listdir(path_category)\n",
    "    # files is a list which contains all the given files' names in the class_type - category\n",
    "    for file_id in files:\n",
    "        # Iterating over each file-id/(each file) in files\n",
    "        total += 1\n",
    "        file = open(path_category + '/' + file_id,'r',encoding=\"ISO-8859-1\")  # Opening each single file\n",
    "        for line in file:\n",
    "            # Iterating over each line in the file\n",
    "            string = str(line)                       # string is str(line)\n",
    "            words = string.strip().split(' ')        # List of all the words in a particular line\n",
    "            if(len(words) == 1 and words[0] == ''):\n",
    "                continue\n",
    "            for word in words:\n",
    "                # Iterating over all the words in a single line\n",
    "                if(word not in stop_words):          # If the word is not in stop_words            \n",
    "                    if(word not in d):               \n",
    "                        d[word] = 1                  # Making the frequency to be 1 if the word is not in the dictionary d  \n",
    "                    else:\n",
    "                        d[word] += 1                 # Updating the frequency if the word is already present in d                 \n",
    "    \n",
    "    \n",
    "    d['total_count'] = total                         # Adding the total_count assosciated with each class_type\n",
    "    total_files += total                             # Adding total to the total number of files \n",
    "    total = 0                                        # Making total again 0 for next iteration\n",
    "    count[category] = d                              # Assosciating the dictionary 'd' to class_type - category   \n",
    "\n",
    "\n",
    "count['total_data'] = total_files                    # Adding the top-level key 'total_data' in count   \n",
    "    \n",
    "count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files/Documents of type alt.atheism : 1000\n",
      "Number of Files/Documents of type comp.graphics : 1000\n",
      "Number of Files/Documents of type comp.os.ms-windows.misc : 1000\n",
      "Number of Files/Documents of type comp.sys.ibm.pc.hardware : 1000\n",
      "Number of Files/Documents of type comp.sys.mac.hardware : 1000\n",
      "Number of Files/Documents of type comp.windows.x : 1000\n",
      "Number of Files/Documents of type misc.forsale : 1000\n",
      "Number of Files/Documents of type rec.autos : 1000\n",
      "Number of Files/Documents of type rec.motorcycles : 1000\n",
      "Number of Files/Documents of type rec.sport.baseball : 1000\n",
      "Number of Files/Documents of type rec.sport.hockey : 1000\n",
      "Number of Files/Documents of type sci.crypt : 1000\n",
      "Number of Files/Documents of type sci.electronics : 1000\n",
      "Number of Files/Documents of type sci.med : 1000\n",
      "Number of Files/Documents of type sci.space : 1000\n",
      "Number of Files/Documents of type soc.religion.christian : 997\n",
      "Number of Files/Documents of type talk.politics.guns : 1000\n",
      "Number of Files/Documents of type talk.politics.mideast : 1000\n",
      "Number of Files/Documents of type talk.politics.misc : 1000\n",
      "Number of Files/Documents of type talk.religion.misc : 1000\n",
      "\n",
      "Total number of Files/Documents are : 19997\n"
     ]
    }
   ],
   "source": [
    "# Printing number of files corresponding to each class_type \n",
    "\n",
    "for key in count:\n",
    "    if(key == 'total_data'):\n",
    "        continue\n",
    "    print('Number of Files/Documents of type',key,':',count[key]['total_count'])\n",
    "\n",
    "print()    \n",
    "print('Total number of Files/Documents are :',count['total_data'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Files/Documents of type alt.atheism : 39835\n",
      "Number of words in Files/Documents of type comp.graphics : 41464\n",
      "Number of words in Files/Documents of type comp.os.ms-windows.misc : 54823\n",
      "Number of words in Files/Documents of type comp.sys.ibm.pc.hardware : 32862\n",
      "Number of words in Files/Documents of type comp.sys.mac.hardware : 30516\n",
      "Number of words in Files/Documents of type comp.windows.x : 47923\n",
      "Number of words in Files/Documents of type misc.forsale : 34505\n",
      "Number of words in Files/Documents of type rec.autos : 34423\n",
      "Number of words in Files/Documents of type rec.motorcycles : 31671\n",
      "Number of words in Files/Documents of type rec.sport.baseball : 33435\n",
      "Number of words in Files/Documents of type rec.sport.hockey : 39625\n",
      "Number of words in Files/Documents of type sci.crypt : 42281\n",
      "Number of words in Files/Documents of type sci.electronics : 33728\n",
      "Number of words in Files/Documents of type sci.med : 45314\n",
      "Number of words in Files/Documents of type sci.space : 46006\n",
      "Number of words in Files/Documents of type soc.religion.christian : 42620\n",
      "Number of words in Files/Documents of type talk.politics.guns : 42911\n",
      "Number of words in Files/Documents of type talk.politics.mideast : 51869\n",
      "Number of words in Files/Documents of type talk.politics.misc : 49967\n",
      "Number of words in Files/Documents of type talk.religion.misc : 44279\n",
      "\n",
      "Total Number of words are : 820057\n"
     ]
    }
   ],
   "source": [
    "# Printing number of words corresponding to each class_type \n",
    "\n",
    "total_words = 0\n",
    "\n",
    "for key in count:\n",
    "    if(key == 'total_data'):\n",
    "        continue\n",
    "    count_ = len(count[key].keys()) - 1\n",
    "    # Number of words in each class = length of count[class_type].keys() - 1  (-1 because of the key 'total_count')\n",
    "    print('Number of words in Files/Documents of type',key,':',count_)\n",
    "    total_words += count_        \n",
    "\n",
    "print()    \n",
    "print('Total Number of words are :',total_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in alt.atheism (with frequency greater than 20) : 1445\n",
      "Number of words in comp.graphics (with frequency greater than 20) : 1168\n",
      "Number of words in comp.os.ms-windows.misc (with frequency greater than 20) : 804\n",
      "Number of words in comp.sys.ibm.pc.hardware (with frequency greater than 20) : 839\n",
      "Number of words in comp.sys.mac.hardware (with frequency greater than 20) : 824\n",
      "Number of words in comp.windows.x (with frequency greater than 20) : 1275\n",
      "Number of words in misc.forsale (with frequency greater than 20) : 676\n",
      "Number of words in rec.autos (with frequency greater than 20) : 923\n",
      "Number of words in rec.motorcycles (with frequency greater than 20) : 889\n",
      "Number of words in rec.sport.baseball (with frequency greater than 20) : 1089\n",
      "Number of words in rec.sport.hockey (with frequency greater than 20) : 1246\n",
      "Number of words in sci.crypt (with frequency greater than 20) : 1588\n",
      "Number of words in sci.electronics (with frequency greater than 20) : 863\n",
      "Number of words in sci.med (with frequency greater than 20) : 1305\n",
      "Number of words in sci.space (with frequency greater than 20) : 1411\n",
      "Number of words in soc.religion.christian (with frequency greater than 20) : 1587\n",
      "Number of words in talk.politics.guns (with frequency greater than 20) : 1550\n",
      "Number of words in talk.politics.mideast (with frequency greater than 20) : 2356\n",
      "Number of words in talk.politics.misc (with frequency greater than 20) : 1912\n",
      "Number of words in talk.religion.misc (with frequency greater than 20) : 1512\n",
      "\n",
      "Total Number of words (with frequency greater than 20) are : 25262\n"
     ]
    }
   ],
   "source": [
    "# Here we are removing all those words from the vocabulary whose frequencies is less than 20\n",
    "\n",
    "for key1 in count:\n",
    "    if(key1 == 'total_data'):\n",
    "        continue\n",
    "    d = count[key1]             # Dictionary corresponding to each class_type\n",
    "    d1 = {}                     # This is the new dictionary that will have all those words with frequency greater than 20  \n",
    "    for key2 in d:\n",
    "        # Iterating over all the words\n",
    "        if(key2 == 'total_count'):\n",
    "            d1['total_count'] = d['total_count']    # Adding the total_count key in d1\n",
    "            continue\n",
    "        if(d[key2] < 20):\n",
    "            continue                  # If freq<20 then no need to add the word in d1\n",
    "        else:\n",
    "            d1[key2] = d[key2]      # Adding the word 'key2' in d1 with freq d[key2] (which is greater than or equal to 20)\n",
    "    count[key1] = d1                # Replacing d with d1 for each class_type\n",
    "    d.clear()                       # Clearing all keys in the d dictionary\n",
    "\n",
    "total_words = 0   \n",
    "for key in count:\n",
    "    if(key == 'total_data'):\n",
    "        continue\n",
    "    count_ = len(count[key].keys()) - 1\n",
    "    print('Number of words in',key,'(with frequency greater than 20)',':',count_)\n",
    "    total_words += count_        \n",
    "\n",
    "print()    \n",
    "print('Total Number of words (with frequency greater than 20) are :',total_words)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25262"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = []                           # List of all possible words which can become our features\n",
    "\n",
    "for key1 in count:\n",
    "    if(key1 == 'total_data'):\n",
    "        continue\n",
    "    d = count[key1]\n",
    "    for key2 in d:\n",
    "        if(key2 == 'total_count'):\n",
    "            continue\n",
    "        features.append(key2)           # Adding each word from dictionary 'd' in the features list\n",
    "\n",
    "# So 'features' list have all those words with frequency greater than or equal to 20        \n",
    "\n",
    "len(features)                           # Same as total_words above       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9093 9093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cantaloupe.srv.cs.cmu.edu',\n",
       " 'mathew',\n",
       " '<mathew@mantis.co.uk>',\n",
       " 'Alt.Atheism',\n",
       " 'Atheist',\n",
       " 'anything',\n",
       " 'related',\n",
       " 'atheism',\n",
       " 'atheism,',\n",
       " 'books,',\n",
       " 'alt.atheism',\n",
       " 'world',\n",
       " 'Mantis',\n",
       " 'Consultants,',\n",
       " 'Cambridge.',\n",
       " 'UK.',\n",
       " 'USA',\n",
       " 'atheist',\n",
       " 'Religion',\n",
       " 'Evolution',\n",
       " 'sell',\n",
       " 'like',\n",
       " 'ones',\n",
       " 'Christians',\n",
       " 'stick',\n",
       " 'word',\n",
       " 'written',\n",
       " 'People',\n",
       " 'San',\n",
       " 'get',\n",
       " 'try',\n",
       " 'people',\n",
       " 'go',\n",
       " 'various',\n",
       " 'books',\n",
       " 'Bible,',\n",
       " 'Biblical',\n",
       " 'One',\n",
       " 'book',\n",
       " 'Bible',\n",
       " 'American',\n",
       " 'contains',\n",
       " 'King',\n",
       " 'James',\n",
       " 'version',\n",
       " 'Bible.',\n",
       " 'including',\n",
       " 'East',\n",
       " 'New',\n",
       " '(which',\n",
       " 'black',\n",
       " 'secular',\n",
       " 'history',\n",
       " 'Americans',\n",
       " 'Society',\n",
       " 'British',\n",
       " 'South',\n",
       " 'Germany',\n",
       " 'write',\n",
       " '1.',\n",
       " 'proof',\n",
       " 'events',\n",
       " 'living',\n",
       " 'dead',\n",
       " 'gods',\n",
       " 'post',\n",
       " 'atomic',\n",
       " 'lives',\n",
       " 'set',\n",
       " 'example,',\n",
       " 'anyone',\n",
       " 'describe',\n",
       " 'use',\n",
       " 'wrote',\n",
       " 'philosophical',\n",
       " 'short',\n",
       " 'stories',\n",
       " 'people,',\n",
       " 'truth',\n",
       " 'religion',\n",
       " 'rather',\n",
       " 'believed',\n",
       " 'sort',\n",
       " 'God,',\n",
       " 'following',\n",
       " 'group',\n",
       " 'Earth',\n",
       " 'women',\n",
       " 'raise',\n",
       " 'faith',\n",
       " 'description',\n",
       " 'religion.',\n",
       " 'Christianity',\n",
       " 'reality',\n",
       " 'possibly',\n",
       " 'God',\n",
       " 'making',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'another',\n",
       " 'system.',\n",
       " 'man',\n",
       " 'whose',\n",
       " 'easy',\n",
       " 'story',\n",
       " 'based',\n",
       " 'premise',\n",
       " 'US',\n",
       " 'take',\n",
       " 'charge',\n",
       " 'nation',\n",
       " 'again.',\n",
       " 'life',\n",
       " 'live',\n",
       " 'new',\n",
       " 'Christian',\n",
       " 'right',\n",
       " 'bank',\n",
       " 'used',\n",
       " 'legal',\n",
       " 'writing',\n",
       " 'difficult',\n",
       " 'somewhat',\n",
       " 'work',\n",
       " 'However,',\n",
       " 'probably',\n",
       " 'worth',\n",
       " 'know',\n",
       " 'about.',\n",
       " 'exists',\n",
       " 'different',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'one',\n",
       " 'true',\n",
       " 'seems',\n",
       " 'even',\n",
       " 'etc.',\n",
       " 'University',\n",
       " 'justification',\n",
       " 'atheism.',\n",
       " 'existence',\n",
       " 'belief',\n",
       " 'great',\n",
       " 'arguments',\n",
       " 'particular',\n",
       " 'theists',\n",
       " 'Case',\n",
       " 'Christianity,',\n",
       " 'best',\n",
       " 'way',\n",
       " 'became',\n",
       " 'alternative',\n",
       " 'particularly',\n",
       " 'religious',\n",
       " 'single',\n",
       " 'statements',\n",
       " 'explicitly',\n",
       " 'present',\n",
       " 'opinions',\n",
       " 'people.',\n",
       " 'popular',\n",
       " 'idea',\n",
       " 'number',\n",
       " 'Great',\n",
       " 'second',\n",
       " 'attempts',\n",
       " 'God.',\n",
       " 'rely',\n",
       " 'upon',\n",
       " 'values',\n",
       " 'simple',\n",
       " 'includes',\n",
       " 'makes',\n",
       " 'attempt',\n",
       " 'moral',\n",
       " 'concept',\n",
       " 'beyond',\n",
       " 'well',\n",
       " 'God\"',\n",
       " 'read',\n",
       " 'less',\n",
       " 'better',\n",
       " 'direct',\n",
       " 'times',\n",
       " 'day',\n",
       " 'above.',\n",
       " 'Atheism',\n",
       " 'study',\n",
       " 'become',\n",
       " 'effect',\n",
       " 'them.',\n",
       " 'small',\n",
       " 'old',\n",
       " 'articles',\n",
       " 'information,',\n",
       " 'send',\n",
       " 'saying',\n",
       " 'help',\n",
       " 'back',\n",
       " 'Please',\n",
       " 'posting',\n",
       " 'article',\n",
       " 'provide',\n",
       " 'general',\n",
       " 'tried',\n",
       " 'possible',\n",
       " 'regarding',\n",
       " 'always',\n",
       " 'remember',\n",
       " 'relevant',\n",
       " 'sense',\n",
       " 'presented',\n",
       " 'questions',\n",
       " 'asked',\n",
       " 'theist',\n",
       " 'since',\n",
       " 'newsgroup',\n",
       " 'answered',\n",
       " 'note',\n",
       " 'towards',\n",
       " 'FAQ',\n",
       " 'actually',\n",
       " 'talk',\n",
       " 'religion,',\n",
       " 'talking',\n",
       " 'religions',\n",
       " 'Islam,',\n",
       " 'discussion',\n",
       " 'apply',\n",
       " 'atheists',\n",
       " 'believe',\n",
       " 'exist.',\n",
       " 'referred',\n",
       " 'position,',\n",
       " 'latter',\n",
       " 'important',\n",
       " 'difference',\n",
       " 'two',\n",
       " 'positive',\n",
       " 'fall',\n",
       " 'assuming',\n",
       " 'others',\n",
       " 'specific',\n",
       " 'thing',\n",
       " 'believing',\n",
       " 'means',\n",
       " 'true.',\n",
       " 'something',\n",
       " 'simply',\n",
       " 'whether',\n",
       " 'us',\n",
       " 'term',\n",
       " 'defined',\n",
       " 'someone',\n",
       " 'things',\n",
       " 'cause',\n",
       " 'believes',\n",
       " 'cannot',\n",
       " 'things,',\n",
       " 'language',\n",
       " 'point',\n",
       " 'view',\n",
       " 'fact',\n",
       " 'mean',\n",
       " 'referring',\n",
       " 'atheists.',\n",
       " 'say',\n",
       " 'certainly',\n",
       " 'case',\n",
       " 'science',\n",
       " 'find',\n",
       " 'basis',\n",
       " 'person',\n",
       " 'atheist,',\n",
       " 'ask',\n",
       " 'Many',\n",
       " 'feel',\n",
       " 'major',\n",
       " 'logically',\n",
       " 'impossible',\n",
       " 'see',\n",
       " 'evidence',\n",
       " 'prove',\n",
       " 'statement.',\n",
       " 'quite',\n",
       " 'exist',\n",
       " 'course,',\n",
       " 'Whether',\n",
       " 'matter',\n",
       " 'still',\n",
       " 'reasons',\n",
       " 'assume',\n",
       " 'exist,',\n",
       " 'show',\n",
       " 'assumption',\n",
       " 'hand',\n",
       " 'question',\n",
       " 'require',\n",
       " 'might',\n",
       " 'there.',\n",
       " 'problem',\n",
       " 'Therefore',\n",
       " 'generally',\n",
       " 'accepted',\n",
       " 'unless',\n",
       " 'Even',\n",
       " 'follow',\n",
       " 'rule',\n",
       " 'strong',\n",
       " 'usually',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'described',\n",
       " 'exists,',\n",
       " 'close',\n",
       " 'every',\n",
       " 'kind',\n",
       " 'really',\n",
       " 'universe',\n",
       " 'way,',\n",
       " 'way.',\n",
       " 'argue',\n",
       " 'all,',\n",
       " 'easily',\n",
       " 'Note',\n",
       " 'physical',\n",
       " 'surely',\n",
       " 'caused',\n",
       " 'think',\n",
       " 'common',\n",
       " 'points',\n",
       " 'true,',\n",
       " 'words',\n",
       " 'statement',\n",
       " 'along',\n",
       " 'agreed',\n",
       " 'uses',\n",
       " 'original',\n",
       " 'definitions',\n",
       " 'agree',\n",
       " 'seen',\n",
       " 'apparently',\n",
       " 'tend',\n",
       " 'play',\n",
       " 'answer',\n",
       " 'depends',\n",
       " 'meant',\n",
       " 'power',\n",
       " 'especially',\n",
       " 'according',\n",
       " 'worship',\n",
       " 'sense.',\n",
       " 'definition',\n",
       " 'result',\n",
       " 'human',\n",
       " 'behaviour',\n",
       " 'science,',\n",
       " 'act',\n",
       " 'faith,',\n",
       " 'entirely',\n",
       " 'clear',\n",
       " 'necessary',\n",
       " 'beliefs',\n",
       " 'subject',\n",
       " 'experience',\n",
       " 'Science',\n",
       " 'assumed',\n",
       " 'laws',\n",
       " 'basic',\n",
       " 'ideas',\n",
       " 'called',\n",
       " 'almost',\n",
       " 'everything',\n",
       " 'said',\n",
       " 'Faith',\n",
       " 'refer',\n",
       " 'certain',\n",
       " 'According',\n",
       " 'faith.',\n",
       " 'individual',\n",
       " 'scientists',\n",
       " 'claiming',\n",
       " 'state',\n",
       " 'without',\n",
       " 'fit',\n",
       " 'beliefs.',\n",
       " 'lack',\n",
       " 'everyone',\n",
       " 'either',\n",
       " 'position',\n",
       " 'sense,',\n",
       " 'believers',\n",
       " 'speak',\n",
       " 'let',\n",
       " 'mention',\n",
       " 'except',\n",
       " 'perhaps',\n",
       " 'part',\n",
       " 'possible.',\n",
       " 'made',\n",
       " 'little',\n",
       " 'society',\n",
       " 'outside',\n",
       " 'church',\n",
       " 'free',\n",
       " 'came',\n",
       " 'Stalin',\n",
       " 'took',\n",
       " 'control',\n",
       " 'order',\n",
       " 'complete',\n",
       " 'matters',\n",
       " 'government',\n",
       " 'concerned',\n",
       " 'allow',\n",
       " 'principle',\n",
       " 'belief.',\n",
       " 'nature.',\n",
       " 'responsible',\n",
       " 'political',\n",
       " 'spending',\n",
       " 'long',\n",
       " 'happy',\n",
       " 'say.',\n",
       " 'care',\n",
       " 'pray',\n",
       " 'Also,',\n",
       " 'told',\n",
       " 'need',\n",
       " 'acceptable',\n",
       " 'public',\n",
       " 'family',\n",
       " 'reasonable',\n",
       " 'time,',\n",
       " 'mentioned',\n",
       " 'object',\n",
       " 'purpose',\n",
       " 'well,',\n",
       " 'good',\n",
       " 'theistic',\n",
       " 'seem',\n",
       " 'them,',\n",
       " 'obvious',\n",
       " \"one's\",\n",
       " 'Perhaps',\n",
       " 'force',\n",
       " 'choose',\n",
       " 'call',\n",
       " 'hand,',\n",
       " 'others.',\n",
       " 'conclude',\n",
       " 'Atheists',\n",
       " 'prefer',\n",
       " 'copy',\n",
       " 'around',\n",
       " 'arguing',\n",
       " 'are,',\n",
       " 'several',\n",
       " 'define',\n",
       " 'morality',\n",
       " 'course',\n",
       " 'morality,',\n",
       " 'within',\n",
       " 'social',\n",
       " 'other.',\n",
       " 'enough',\n",
       " 'reason',\n",
       " 'natural',\n",
       " 'happens',\n",
       " 'know,',\n",
       " 'justify',\n",
       " 'example:',\n",
       " 'full',\n",
       " 'Jesus',\n",
       " 'Christ',\n",
       " 'save',\n",
       " 'shown',\n",
       " 'example',\n",
       " 'life.',\n",
       " 'quote',\n",
       " 'court',\n",
       " 'found',\n",
       " 'behavior',\n",
       " 'done',\n",
       " 'percent',\n",
       " 'sex',\n",
       " 'yes,',\n",
       " 'explained',\n",
       " 'least',\n",
       " 'held',\n",
       " 'imply',\n",
       " 'created',\n",
       " 'humans',\n",
       " 'seeing',\n",
       " 'rules',\n",
       " 'supernatural',\n",
       " 'observed',\n",
       " 'majority',\n",
       " 'time',\n",
       " 'sometimes',\n",
       " 'considered',\n",
       " 'reject',\n",
       " 'choice',\n",
       " 'want',\n",
       " 'nobody',\n",
       " 'figure',\n",
       " 'able',\n",
       " 'merely',\n",
       " 'wants',\n",
       " 'approach',\n",
       " 'decide',\n",
       " 'possibility',\n",
       " 'trying',\n",
       " 'open',\n",
       " 'looking',\n",
       " 'likely',\n",
       " 'wish',\n",
       " 'debate',\n",
       " 'give',\n",
       " 'doubt',\n",
       " 'willing',\n",
       " 'basically',\n",
       " 'telling',\n",
       " 'truth,',\n",
       " 'whole',\n",
       " 'completely',\n",
       " 'gives',\n",
       " 'meaning',\n",
       " 'life,',\n",
       " 'hope',\n",
       " 'look',\n",
       " 'put',\n",
       " 'looks',\n",
       " 'asking',\n",
       " 'silly',\n",
       " 'sound',\n",
       " 'face',\n",
       " 'otherwise',\n",
       " 'consider',\n",
       " 'hard',\n",
       " 'years.',\n",
       " 'supposed',\n",
       " 'years',\n",
       " 'wrong',\n",
       " 'stop',\n",
       " 'real',\n",
       " 'harm',\n",
       " 'else.',\n",
       " 'burden',\n",
       " 'money',\n",
       " 'effort',\n",
       " 'died',\n",
       " 'countries',\n",
       " 'known',\n",
       " 'murder',\n",
       " 'children',\n",
       " 'claimed',\n",
       " 'convinced',\n",
       " 'kill',\n",
       " 'Church',\n",
       " 'dogma',\n",
       " 'Christian.',\n",
       " 'Maybe',\n",
       " 'support',\n",
       " 'interpretation',\n",
       " 'Sorry,',\n",
       " 'so,',\n",
       " 'ever',\n",
       " 'pointed',\n",
       " 'start',\n",
       " 'valid',\n",
       " 'hold',\n",
       " 'assertion',\n",
       " 'nothing',\n",
       " 'deal',\n",
       " 'that.',\n",
       " 'perfectly',\n",
       " 'started',\n",
       " 'explain',\n",
       " 'well.',\n",
       " 'philosopher',\n",
       " 'god,',\n",
       " 'already',\n",
       " 'out,',\n",
       " 'realize',\n",
       " 'special',\n",
       " 'theory',\n",
       " 'using',\n",
       " 'keep',\n",
       " 'fundamental',\n",
       " 'change',\n",
       " 'good.',\n",
       " 'question.',\n",
       " 'taking',\n",
       " 'information',\n",
       " 'I3150101@dbstu1.rz.tu-bs.de',\n",
       " '(Benedikt',\n",
       " 'Rosenau)',\n",
       " 'postnntp@ibr.cs.tu-bs.de',\n",
       " '(Mr.',\n",
       " 'Nntp',\n",
       " 'Inews',\n",
       " 'Entry)',\n",
       " 'Technical',\n",
       " 'Braunschweig,',\n",
       " 'mangoe@cs.umd.edu',\n",
       " '(Charley',\n",
       " 'Wingate)',\n",
       " 'John',\n",
       " 'necessarily',\n",
       " 'argument',\n",
       " 'me.',\n",
       " 'quotes',\n",
       " 'appear',\n",
       " 'Matthew',\n",
       " 'similar',\n",
       " 'knowledge',\n",
       " 'knew',\n",
       " 'Luke',\n",
       " 'obviously',\n",
       " 'case.',\n",
       " 'texts',\n",
       " 'here,',\n",
       " 'explanation',\n",
       " 'says',\n",
       " 'text',\n",
       " 'got',\n",
       " 'early',\n",
       " 'rest',\n",
       " 'point,',\n",
       " 'exactly',\n",
       " 'base',\n",
       " 'Mark',\n",
       " \"what's\",\n",
       " 'step',\n",
       " 'source',\n",
       " 'directly',\n",
       " 'news',\n",
       " 'gets',\n",
       " 'this.',\n",
       " 'IS',\n",
       " 'gospel',\n",
       " 'itself.',\n",
       " 'generation',\n",
       " 'bad',\n",
       " 'material',\n",
       " 'verse',\n",
       " 'pretty',\n",
       " 'clearly',\n",
       " 'later',\n",
       " 'end',\n",
       " 'words,',\n",
       " 'Benedikt',\n",
       " 'X-Newsreader:',\n",
       " 'rusnews',\n",
       " 'none',\n",
       " 'next',\n",
       " 'heard',\n",
       " 'defend',\n",
       " 'sounds',\n",
       " 'run',\n",
       " 'worse',\n",
       " 'rights',\n",
       " 'issues',\n",
       " 'contradiction',\n",
       " 'jbrown@batman.bmd.trw.com',\n",
       " 'perfect',\n",
       " 'terms',\n",
       " 'leads',\n",
       " 'understand',\n",
       " 'coming',\n",
       " 'atheist.',\n",
       " 'time.',\n",
       " 'shows',\n",
       " 'away',\n",
       " 'ability',\n",
       " 'error',\n",
       " 'first',\n",
       " 'evil',\n",
       " 'one.',\n",
       " 'opinion,',\n",
       " 'happen',\n",
       " 'place',\n",
       " 'omniscient',\n",
       " 'conscious',\n",
       " 'god',\n",
       " 'beings',\n",
       " 'greater',\n",
       " 'knowing',\n",
       " 'system',\n",
       " 'good,',\n",
       " 'possible,',\n",
       " 'truly',\n",
       " 'said,',\n",
       " 'consistent',\n",
       " '(Deletion)',\n",
       " 'reading',\n",
       " 'logic',\n",
       " 'amount',\n",
       " 'fair',\n",
       " 'bible',\n",
       " 'lot',\n",
       " 'interested',\n",
       " 'keith@cco.caltech.edu',\n",
       " '(Keith',\n",
       " 'Allan',\n",
       " 'Schneider)',\n",
       " 'Atheists?',\n",
       " 'California',\n",
       " 'Institute',\n",
       " 'Technology,',\n",
       " 'Pasadena',\n",
       " 'NNTP-Posting-Host:',\n",
       " 'punisher.caltech.edu',\n",
       " 'arromdee@jyusenkyou.cs.jhu.edu',\n",
       " '(Ken',\n",
       " 'Arromdee)',\n",
       " 'motto',\n",
       " 'required',\n",
       " 'keith',\n",
       " 'Islam',\n",
       " 'jaeger@buphy.bu.edu',\n",
       " '(Gregg',\n",
       " 'Jaeger)',\n",
       " 'Muslims',\n",
       " 'all.',\n",
       " 'muslim',\n",
       " 'attack',\n",
       " 'situation',\n",
       " 'Muslim',\n",
       " '<Political',\n",
       " 'line',\n",
       " 'livesey@solntze.wpd.sgi.com',\n",
       " '(Jon',\n",
       " 'Livesey)',\n",
       " '\"objective\"',\n",
       " 'admit',\n",
       " 'morality.',\n",
       " 'form',\n",
       " 'morals',\n",
       " 'Well,',\n",
       " 'goal',\n",
       " 'Another',\n",
       " 'system,',\n",
       " 'unto',\n",
       " 'And,',\n",
       " 'thought',\n",
       " 'killing',\n",
       " 'capital',\n",
       " 'punishment',\n",
       " 'fail',\n",
       " 'anything.',\n",
       " 'never',\n",
       " 'objective',\n",
       " 'claim.',\n",
       " 'wrong.',\n",
       " 'current',\n",
       " 'objectively',\n",
       " 'Keith',\n",
       " 'personal',\n",
       " 'wrong,',\n",
       " 'implies',\n",
       " 'kmr4@po.CWRU.edu',\n",
       " 'Ryan)',\n",
       " 'becomes',\n",
       " 'notion',\n",
       " 'account',\n",
       " 'population',\n",
       " 'sandvik@newton.apple.com',\n",
       " '(Kent',\n",
       " 'Sandvik)',\n",
       " 'But,',\n",
       " 'judge',\n",
       " 'discuss',\n",
       " 'alt.atheism,talk.religion.misc,talk.origins',\n",
       " 'Albert',\n",
       " 'Sabin',\n",
       " '(Bill',\n",
       " 'ad',\n",
       " 'come',\n",
       " 'creation',\n",
       " 'right,',\n",
       " 'determine',\n",
       " 'again,',\n",
       " 'belief,',\n",
       " 'psychological',\n",
       " 'poor',\n",
       " 'one,',\n",
       " 'bobbe@vice.ICO.TEK.COM',\n",
       " '(Robert',\n",
       " 'Beauchaine)',\n",
       " 'continue',\n",
       " 'going',\n",
       " 'killed',\n",
       " 'Yes,',\n",
       " 'thinks',\n",
       " 'involved',\n",
       " 'action',\n",
       " 'getting',\n",
       " 'bit',\n",
       " 'works',\n",
       " 'innocent',\n",
       " 'value',\n",
       " 'solely',\n",
       " 'needs',\n",
       " 'risk',\n",
       " 'die',\n",
       " 'factor',\n",
       " 'Anyway,',\n",
       " 'views',\n",
       " 'local',\n",
       " 'him.',\n",
       " 'entire',\n",
       " 'mean,',\n",
       " 'committed',\n",
       " 'death',\n",
       " 'penalty',\n",
       " 'due',\n",
       " 'least,',\n",
       " 'absolutely',\n",
       " 'member',\n",
       " 'convince',\n",
       " 'accept',\n",
       " 'Inc.,',\n",
       " 'Beaverton,',\n",
       " '(James',\n",
       " 'higher',\n",
       " 'E',\n",
       " 'indeed',\n",
       " 'up.',\n",
       " 'Bob',\n",
       " 'Beauchaine',\n",
       " 'Queens',\n",
       " 'stay,',\n",
       " 'blew',\n",
       " 'Bronx',\n",
       " 'away,',\n",
       " 'sank',\n",
       " 'Manhattan',\n",
       " 'sea.',\n",
       " 'islamic',\n",
       " 'darice@yoyo.cc.monash.edu.au',\n",
       " '(Fred',\n",
       " 'Rice)',\n",
       " 'level',\n",
       " 'mathematical',\n",
       " 'right.',\n",
       " 'however,',\n",
       " 'brought',\n",
       " 'large',\n",
       " 'deaths',\n",
       " 'practice',\n",
       " 'society,',\n",
       " 'respond',\n",
       " 'thread',\n",
       " 'probability',\n",
       " 'fine',\n",
       " 'Computer',\n",
       " 'king@ctron.com',\n",
       " '(John',\n",
       " 'King)',\n",
       " 'totally',\n",
       " 'tells',\n",
       " 'behind',\n",
       " 'wonder',\n",
       " 'net.',\n",
       " 'type',\n",
       " 'creationist',\n",
       " 'appears',\n",
       " 'facts',\n",
       " 'taken',\n",
       " 'you?',\n",
       " 'post.',\n",
       " 'past',\n",
       " 'high',\n",
       " 'Read',\n",
       " 'universe,',\n",
       " 'intended',\n",
       " 'science.',\n",
       " 'fact,',\n",
       " 'creationism',\n",
       " 'evolution',\n",
       " 'you.',\n",
       " 'list',\n",
       " 'contradictions',\n",
       " '(David',\n",
       " 'Since',\n",
       " 'verses',\n",
       " 'context,',\n",
       " 'given',\n",
       " 'false',\n",
       " 'prophecy',\n",
       " 'context',\n",
       " 'thrown',\n",
       " 'cut',\n",
       " 'useful',\n",
       " 'halat@pooh.bears',\n",
       " '(Jim',\n",
       " 'Halat)',\n",
       " 'scientific',\n",
       " 'genetic',\n",
       " 'create',\n",
       " 'interesting',\n",
       " 'light',\n",
       " 'understanding',\n",
       " 'halat',\n",
       " 'Dan',\n",
       " '(Usenet',\n",
       " 'News)',\n",
       " 'Ann',\n",
       " 'Miller',\n",
       " 'chance',\n",
       " 'Wingate',\n",
       " 'tell',\n",
       " 'actions',\n",
       " 'law',\n",
       " 'Again,',\n",
       " 'actual',\n",
       " 'say,',\n",
       " 'opinion',\n",
       " 'include',\n",
       " 'thing,',\n",
       " 'not,',\n",
       " 'consequences',\n",
       " 'systems',\n",
       " 'case,',\n",
       " 'thinking',\n",
       " 'minds',\n",
       " 'absolute',\n",
       " 'method',\n",
       " 'truth.',\n",
       " 'comes',\n",
       " 'then,',\n",
       " '(if',\n",
       " 'degree',\n",
       " 'know.',\n",
       " 'fairly',\n",
       " 'reality.',\n",
       " 'Death',\n",
       " 'working',\n",
       " 'recognize',\n",
       " 'up,',\n",
       " 'requires',\n",
       " 'instead',\n",
       " 'country',\n",
       " 'theory.',\n",
       " 'on,',\n",
       " 'falls',\n",
       " 'College',\n",
       " 'Nntp-Posting-Host:',\n",
       " 'deleted]',\n",
       " 'postings',\n",
       " 'add',\n",
       " 'nice',\n",
       " 'University,',\n",
       " 'authority',\n",
       " 'sgi',\n",
       " 'solntze.wpd.sgi.com',\n",
       " 'snm6394@ultb.isc.rit.edu',\n",
       " '(S.N.',\n",
       " 'Mozumder',\n",
       " 'jon.',\n",
       " 'reliable',\n",
       " 'issue',\n",
       " 'earlier',\n",
       " 'please',\n",
       " 'decided',\n",
       " 'groups',\n",
       " 'so.',\n",
       " '****************************************************************',\n",
       " 'Inc.',\n",
       " 'Illinois',\n",
       " 'contain',\n",
       " 'leave',\n",
       " 'serve',\n",
       " 'among',\n",
       " 'three',\n",
       " 'middle',\n",
       " 'leaders',\n",
       " 'takes',\n",
       " 'GO',\n",
       " 'message',\n",
       " 'starting',\n",
       " 'acooper@mac.cc.macalstr.edu',\n",
       " 'fellow',\n",
       " '-0600',\n",
       " 'drawn',\n",
       " 'Adam',\n",
       " 'weak',\n",
       " 'evidence.',\n",
       " 'NOT',\n",
       " 'I,',\n",
       " 'space',\n",
       " 'observations',\n",
       " 'conclusion',\n",
       " 'precisely',\n",
       " 'neither',\n",
       " 'disagree',\n",
       " 'name',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set = []                        # This will be our final feature_set\n",
    "\n",
    "\n",
    "# Adding all unique entries from the 'features' list above into the list 'feature_set'\n",
    "# As 'features' list might contain duplicate entries\n",
    "\n",
    "for i in features:\n",
    "    if(i not in feature_set):\n",
    "        feature_set.append(i)\n",
    "        \n",
    "print(len(feature_set),len(set(feature_set)))\n",
    "# feature_set                           # Final feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the structure of X_train\n",
    "# And Creating Y_train\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.zeros( ( count['total_data'], len(feature_set) ) )\n",
    "# X_train will have count['total_data'] i.e 19997 rows and len(feature_set) i.e 9093 columns\n",
    "\n",
    "\n",
    "Y_train = []\n",
    "fileID = []                     # A list comprising of all the file_ID/file_names\n",
    "\n",
    "# fileID will help us to find the index corresponding to each file_name.So that we can fill our X_train\n",
    "# Let's say file_name = '1200' is present at index 20 in fileID \n",
    "# then whenever we will be iterating over all the words in file '1200',all the frequency updates will be made in the row(20) of X_train\n",
    "# Similarly feature_set will help us to find the corresponding column index in X_train where frequency updates are to be made\n",
    "# Let's suppose we have a word 'ABC' in feature_set\n",
    "# And if 'ABC' appears in suppose file '123'\n",
    "# then i = fileID.index(123) and j = feature_set.index(ABC)\n",
    "# and we will update the freq at X_train[i,j]\n",
    "\n",
    "\n",
    "\n",
    "path_20_newsgrp = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Training data)/20_newsgroups'\n",
    "categories = os.listdir(path_20_newsgrp)\n",
    "\n",
    "for category in categories:\n",
    "    path_category = path_20_newsgrp + '/' + category\n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        Y_train.append(category)\n",
    "        # Adding category/class_type of each file to Y_train\n",
    "        fileID.append(file_id) \n",
    "        # Adding each file_id/file_name to list fileID\n",
    "\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 9093) (19997,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "set(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling values in X_train\n",
    "\n",
    "path_20_newsgrp = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Training data)/20_newsgroups'\n",
    "categories = os.listdir(path_20_newsgrp)\n",
    "\n",
    "for category in categories:\n",
    "    path_category = path_20_newsgrp + '/' + category\n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        file = open(path_category + '/' + file_id,'r',encoding=\"ISO-8859-1\")\n",
    "        for line in file:\n",
    "            string = str(line)\n",
    "            words = string.strip().split(' ')\n",
    "            for word in words:\n",
    "                # Iterating over each word in the file\n",
    "                if(word in feature_set):    \n",
    "                    i = fileID.index(file_id)             # i gives the row index in X_train\n",
    "                    j = feature_set.index(word)           # j gives the column index in X_train\n",
    "                    X_train[i][j] += 1                    # Adding the frequency at the corresponding cell[i,j] in X_train\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_train(For Text Classification).csv',X_train,fmt = '%d')\n",
    "\n",
    "# Since filling values in X_train takes a lot of time so we are just saving it in an external file.\n",
    "# So that the next time we can just use\n",
    "# X_train = np.loadtxt('X_train(For Text Classification).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9083</th>\n",
       "      <th>9084</th>\n",
       "      <th>9085</th>\n",
       "      <th>9086</th>\n",
       "      <th>9087</th>\n",
       "      <th>9088</th>\n",
       "      <th>9089</th>\n",
       "      <th>9090</th>\n",
       "      <th>9091</th>\n",
       "      <th>9092</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   9083  \\\n",
       "0   1.0   2.0   1.0   1.0   5.0   1.0   1.0   1.0   2.0   2.0  ...    0.0   \n",
       "1   1.0   3.0   2.0   1.0   1.0   1.0   0.0  23.0   2.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   1.0   2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   9084  9085  9086  9087  9088  9089  9090  9091  9092  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 9093 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantaloupe.srv.cs.cmu.edu</th>\n",
       "      <th>mathew</th>\n",
       "      <th>&lt;mathew@mantis.co.uk&gt;</th>\n",
       "      <th>Alt.Atheism</th>\n",
       "      <th>Atheist</th>\n",
       "      <th>anything</th>\n",
       "      <th>related</th>\n",
       "      <th>atheism</th>\n",
       "      <th>atheism,</th>\n",
       "      <th>books,</th>\n",
       "      <th>...</th>\n",
       "      <th>\"Jehovah\"</th>\n",
       "      <th>(Ps.</th>\n",
       "      <th>Ps.</th>\n",
       "      <th>(emphasis</th>\n",
       "      <th>added)</th>\n",
       "      <th>[Jehovah]</th>\n",
       "      <th>[Jehovah],</th>\n",
       "      <th>[Elohim],</th>\n",
       "      <th>[Elohim]</th>\n",
       "      <th>||Sun||</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cantaloupe.srv.cs.cmu.edu  mathew  <mathew@mantis.co.uk>  Alt.Atheism  \\\n",
       "0                        1.0     2.0                    1.0          1.0   \n",
       "1                        1.0     3.0                    2.0          1.0   \n",
       "2                        0.0     0.0                    0.0          0.0   \n",
       "3                        1.0     2.0                    1.0          0.0   \n",
       "4                        1.0     0.0                    0.0          0.0   \n",
       "\n",
       "   Atheist  anything  related  atheism  atheism,  books,   ...     \"Jehovah\"  \\\n",
       "0      5.0       1.0      1.0      1.0       2.0     2.0   ...           0.0   \n",
       "1      1.0       1.0      0.0     23.0       2.0     0.0   ...           0.0   \n",
       "2      0.0       0.0      0.0      0.0       0.0     0.0   ...           0.0   \n",
       "3      0.0       0.0      0.0      0.0       0.0     0.0   ...           0.0   \n",
       "4      0.0       0.0      0.0      0.0       0.0     0.0   ...           0.0   \n",
       "\n",
       "   (Ps.  Ps.  (emphasis  added)  [Jehovah]  [Jehovah],  [Elohim],  [Elohim]  \\\n",
       "0   0.0  0.0        0.0     0.0        0.0         0.0        0.0       0.0   \n",
       "1   0.0  0.0        0.0     0.0        0.0         0.0        0.0       0.0   \n",
       "2   0.0  0.0        0.0     0.0        0.0         0.0        0.0       0.0   \n",
       "3   0.0  0.0        0.0     0.0        0.0         0.0        0.0       0.0   \n",
       "4   0.0  0.0        0.0     0.0        0.0         0.0        0.0       0.0   \n",
       "\n",
       "   ||Sun||  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 9093 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are trying to see that whether we have any column in X_train with all 0 entries or not\n",
    "\n",
    "count_ = 0\n",
    "for i in df.columns:\n",
    "    if(df[df.columns[i]].sum() == 0):\n",
    "        count_ += 1\n",
    "\n",
    "print(count_) \n",
    "df.columns = feature_set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 9093) (2000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = []\n",
    "fileId = []               # fileid list for creating X_test   \n",
    "Y_test = []\n",
    "\n",
    "path_mini_newsgrp = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Testing data)/mini_newsgroups'\n",
    "# This is the path of the root folder for the (TESTING DATA)\n",
    "# (Which contains all the other 20 folders of different classes)\n",
    "# Each of these 20 folders comprises of all the files or the documents\n",
    "# The total number of files are 2000\n",
    "\n",
    "\n",
    "# Below code is same as above\n",
    "\n",
    "categories = os.listdir(path_mini_newsgrp)\n",
    "\n",
    "for category in categories:\n",
    "    path_category = path_mini_newsgrp + '/' + category\n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        X_test.append([0 for i in range(len(feature_set))]) \n",
    "        # Here for each file(which is ROW) we are adding a [ list of ( len(feature_set) ) 0s ] (which are COLUMNS)\n",
    "        Y_test.append(category)\n",
    "        fileId.append(file_id)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "set(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling values in X_test\n",
    "# Using the same method as in X_train\n",
    "\n",
    "path_mini_news_group = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Testing Data)/mini_newsgroups'\n",
    "categories = os.listdir(path_mini_news_group)\n",
    "for category in categories:\n",
    "    path_category = path_mini_news_group + '/' + category\n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        file = open(path_category + '/' + file_id ,'r',encoding=\"ISO-8859-1\")\n",
    "        for line in file:\n",
    "            string = str(line)\n",
    "            words = string.strip().split(' ')\n",
    "            for word in words:\n",
    "                if(word in feature_set):\n",
    "                    i = fileId.index(file_id)\n",
    "                    j = feature_set.index(word)\n",
    "                    X_test[i][j] += 1                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_test(For Text Classification).csv',X_test,fmt = '%d')\n",
    "\n",
    "# Since filling values in X_test takes a lot of time so we are just saving it in an external file.\n",
    "# So that the next time we can just use\n",
    "# X_test = np.loadtxt('X_test(For Text Classification).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Training data using Sklearn MultinomialNB classifier : 0.7418612791918788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_predicted_train = clf.predict(X_train)\n",
    "print( 'Score on Training data using Sklearn MultinomialNB classifier :', clf.score(X_train,Y_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.17      0.77      0.28      1000\n",
      "           comp.graphics       0.90      0.93      0.91      1000\n",
      " comp.os.ms-windows.misc       0.89      0.97      0.92      1000\n",
      "comp.sys.ibm.pc.hardware       0.89      0.90      0.90      1000\n",
      "   comp.sys.mac.hardware       0.92      0.98      0.95      1000\n",
      "          comp.windows.x       0.97      0.94      0.95      1000\n",
      "            misc.forsale       0.92      0.81      0.86      1000\n",
      "               rec.autos       0.95      0.97      0.96      1000\n",
      "         rec.motorcycles       0.97      0.86      0.91      1000\n",
      "      rec.sport.baseball       0.96      0.23      0.38      1000\n",
      "        rec.sport.hockey       0.89      0.67      0.76      1000\n",
      "               sci.crypt       0.99      0.96      0.98      1000\n",
      "         sci.electronics       0.84      0.19      0.31      1000\n",
      "                 sci.med       0.95      0.87      0.91      1000\n",
      "               sci.space       0.93      0.45      0.61      1000\n",
      "  soc.religion.christian       0.96      1.00      0.98       997\n",
      "      talk.politics.guns       0.76      0.49      0.60      1000\n",
      "   talk.politics.mideast       0.77      0.29      0.43      1000\n",
      "      talk.politics.misc       0.89      0.77      0.82      1000\n",
      "      talk.religion.misc       0.78      0.80      0.79      1000\n",
      "\n",
      "               micro avg       0.74      0.74      0.74     19997\n",
      "               macro avg       0.86      0.74      0.76     19997\n",
      "            weighted avg       0.86      0.74      0.76     19997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_train,Y_predicted_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Testing data using Sklearn MultinomialNB classifier : 0.78\n"
     ]
    }
   ],
   "source": [
    "Y_predicted_test = clf.predict(X_test)\n",
    "print( 'Score on Testing data using Sklearn MultinomialNB classifier :' ,clf.score(X_test,Y_test) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.55      0.72      0.62       100\n",
      "           comp.graphics       0.86      0.95      0.90       100\n",
      " comp.os.ms-windows.misc       0.80      0.94      0.86       100\n",
      "comp.sys.ibm.pc.hardware       0.66      0.84      0.74       100\n",
      "   comp.sys.mac.hardware       0.76      0.96      0.85       100\n",
      "          comp.windows.x       0.97      0.95      0.96       100\n",
      "            misc.forsale       0.55      0.66      0.60       100\n",
      "               rec.autos       0.84      0.98      0.90       100\n",
      "         rec.motorcycles       0.59      0.88      0.71       100\n",
      "      rec.sport.baseball       0.97      0.34      0.50       100\n",
      "        rec.sport.hockey       0.76      0.95      0.84       100\n",
      "               sci.crypt       0.97      0.94      0.95       100\n",
      "         sci.electronics       0.78      0.35      0.48       100\n",
      "                 sci.med       0.98      0.96      0.97       100\n",
      "               sci.space       0.96      0.64      0.77       100\n",
      "  soc.religion.christian       0.96      1.00      0.98       100\n",
      "      talk.politics.guns       0.79      0.70      0.74       100\n",
      "   talk.politics.mideast       0.84      0.31      0.45       100\n",
      "      talk.politics.misc       0.81      0.74      0.77       100\n",
      "      talk.religion.misc       0.70      0.79      0.74       100\n",
      "\n",
      "               micro avg       0.78      0.78      0.78      2000\n",
      "               macro avg       0.80      0.78      0.77      2000\n",
      "            weighted avg       0.80      0.78      0.77      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_predicted_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWN IMPLEMENTATION OF TEXT CLASSIFICATION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(file_path,feature_set,count,class_value):\n",
    "    \n",
    "    # This function will give the probability that a file belongs to the class-'class_value'\n",
    "    # It takes in the path of the file for which the probability is to be calculated\n",
    "    # It takes in feature_set,vocabulary-'count' and the class_value corresponding to which the probability is to be calculated\n",
    "    \n",
    "    \n",
    "    # Here we will be finding\n",
    "    # P(X = x / y = ai)*P(y = ai)\n",
    "    \n",
    "    class_prob = np.log(count[class_value]['total_count']/count['total_data'])    # log( P(y = ai) )\n",
    "        \n",
    "    file = open(file_path,'r',encoding=\"ISO-8859-1\")\n",
    "    # Opening the file\n",
    "    \n",
    "    \n",
    "    # Now we will find P(X = x / y = ai)\n",
    "    \n",
    "    for line in file:\n",
    "        # Iterating over each line in the file\n",
    "        string = str(line)\n",
    "        words = string.strip().split(' ')\n",
    "        for word in words:\n",
    "            # Iterating over all the words in each line\n",
    "            if(word in feature_set):\n",
    "                # If the word is in feature_set\n",
    "                if(word in count[class_value]):\n",
    "                    # If the word belongs to the class-'class_value' in the vocabulary-'count'\n",
    "                    f = count[class_value][word]                                # Frequency of the word\n",
    "                else:\n",
    "                    # If the word does not belong to the class-'class_value' in the vocabulary-'count'\n",
    "                    f = 0                                                       # Frequency of the word\n",
    "                \n",
    "                numerator = np.log( f + 1 )                                     # +1 for laplace correction \n",
    "                # numerator gives the count of word appearing in class-'class_value'\n",
    "                \n",
    "                all_freq = list( count[class_value].values() )\n",
    "                all_words = sum(all_freq) - count[class_value]['total_count']\n",
    "                denominator = np.log( all_words   +   len(feature_set)   )\n",
    "                # did +len(feature_set) for Laplace correction\n",
    "                # denominator gives the count of all words appearing in class-'class_value'\n",
    "                \n",
    "                prob = numerator - denominator                                  # P(X=x/y=ai)\n",
    "                \n",
    "                class_prob += prob                                              # P(X=x/y=ai) * P(y=ai)\n",
    "                \n",
    "            else:\n",
    "                # If word not in feature_set\n",
    "                continue\n",
    "                \n",
    "    return class_prob                                                           # Returning the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneFile(file_path,feature_set,count,categories):\n",
    "    \n",
    "    # This function will predict the class/category for one file \n",
    "    # It takes in the path of the file (whose class is to be determined)\n",
    "    # The feature_set,the vocabulary 'count' and all the possible class_types/categories\n",
    "    \n",
    "    best_class = ''                  # Best class is the class to which the file belongs to\n",
    "    max_probability = -1000          # Probability corresponding to the best_class        \n",
    "    first_run = True                 # For updating best_class and max_probability at first run always\n",
    "    \n",
    "    for class_value in categories:\n",
    "        # Iterating over all the possible categories\n",
    "        prob = probability(file_path,feature_set,count,class_value)\n",
    "        # probability() will give us the probability that the file belongs to the class_type - category\n",
    "        # We will find this prob for all the 20 classes\n",
    "        # And will find the best class as the class corresponding to which prob is maximum\n",
    "        \n",
    "        if( first_run or (prob > max_probability) ):         # Finding max_probability    \n",
    "            max_probability = prob\n",
    "            best_class = class_value                         # Updating the best class\n",
    "        first_run = False                                    # Making first_run False after the first iteration\n",
    "    \n",
    "    return best_class                                        # returning the best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path_root_folder,feature_set,count):\n",
    "    \n",
    "    # This function will give the predictions for each of the given file/document\n",
    "    # It takes in the path of the root_folder which stores all the other 20 folders which have all the files/documents\n",
    "    # FIRST I THOUGHT OF PASSING IT THE PATH OF A FOLDER WHICH HAS ALL THE FILES ONLY\n",
    "    # SO THAT I WOULD NOT KNOW THE CATEGORY OF EACH FILE BEFOREHAND\n",
    "    # BUT THAT APPROACH HAD ONE ISSUE : WHEN I COPY-PASTED ALL TEST FILES IN ONE SUCH FOLDER\n",
    "    # THEN THEY ALL GOT SHUFFELED ON THE BASIS OF THEIR NAMES/FILE-ID\n",
    "    # HENCE THE Y_TEST I AM HAVING WILL NO LONGER RESEMBLE THE CLASSES OF THESE FILES (AS THE ORDER HAS BEEN CHANGED)\n",
    "    # HENCE IT WON'T BE POSSIBLE FOR US TO COMPARE Y_PRED WITH Y_TEST FOR ACCURACY PURPOSE\n",
    "    # SO UNFORTUNATELY I COULDN'T DO THAT\n",
    "    # This function takes 2 more arguments as feature_set(List of all the features) \n",
    "    # and the vocabulary 'count'\n",
    "    \n",
    "    Y_predicted = []\n",
    "    \n",
    "    categories = os.listdir(path_root_folder)\n",
    "    # list of all the folders inside the root_folder\n",
    "    for category in categories:\n",
    "        path_category = path_root_folder + '/' + category\n",
    "        files = os.listdir(path_category)\n",
    "        for file_id in files:\n",
    "            # Iterating over each file\n",
    "            file_path = path_category + '/' + file_id                           # Path to each file \n",
    "            file_class = predictOneFile(file_path,feature_set,count,categories) \n",
    "            # predictOneFile() will give me the class of each file/the category it belongs to\n",
    "            Y_predicted.append(file_class)\n",
    "            # appending the predicted class to Y_predicted\n",
    "    \n",
    "    Y_predicted = np.array(Y_predicted)\n",
    "    return Y_predicted                                                          # Returning the predictions               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the predict function on training data\n",
    "path = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Testing Data)/mini_newsgroups'\n",
    "Y_predicted_test1 = predict(path,feature_set,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Testing Data by using our own TextClassification code : 0.873\n"
     ]
    }
   ],
   "source": [
    "count_ = 0\n",
    "for i in range(len(Y_predicted_test1)):\n",
    "    if(Y_predicted_test1[i] == Y_test[i]):\n",
    "        count_ += 1  \n",
    "\n",
    "print('Score on Testing Data by using our own TextClassification code :',count_/len(Y_predicted_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.79      0.84      0.81       100\n",
      "           comp.graphics       0.79      0.85      0.82       100\n",
      " comp.os.ms-windows.misc       0.82      0.91      0.86       100\n",
      "comp.sys.ibm.pc.hardware       0.82      0.85      0.83       100\n",
      "   comp.sys.mac.hardware       0.88      0.88      0.88       100\n",
      "          comp.windows.x       0.94      0.87      0.90       100\n",
      "            misc.forsale       0.88      0.79      0.83       100\n",
      "               rec.autos       0.95      0.88      0.91       100\n",
      "         rec.motorcycles       0.96      0.95      0.95       100\n",
      "      rec.sport.baseball       0.96      0.93      0.94       100\n",
      "        rec.sport.hockey       0.94      0.97      0.96       100\n",
      "               sci.crypt       0.93      0.90      0.91       100\n",
      "         sci.electronics       0.86      0.95      0.90       100\n",
      "                 sci.med       0.97      0.92      0.94       100\n",
      "               sci.space       0.95      0.95      0.95       100\n",
      "  soc.religion.christian       0.92      0.98      0.95       100\n",
      "      talk.politics.guns       0.82      0.81      0.81       100\n",
      "   talk.politics.mideast       0.93      0.96      0.95       100\n",
      "      talk.politics.misc       0.68      0.71      0.70       100\n",
      "      talk.religion.misc       0.69      0.56      0.62       100\n",
      "\n",
      "               micro avg       0.87      0.87      0.87      2000\n",
      "               macro avg       0.87      0.87      0.87      2000\n",
      "            weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_predicted_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE :\n",
    "\n",
    "The following is the code i talked about in Cell-6 from above\n",
    "If you want,you can have a look at it\n",
    "Or can ignore it and Evaluate the above code only\n",
    "\n",
    "\n",
    "In the following code only change is in the stop_words list and the way we are reading each line and words \n",
    "i.e line = re.sub(\"[^a-zA-Z ]\",\" \",line)\n",
    "and word = word.lower()\n",
    "Everything rest is exactly the same.So i have not provided any comments.But i have highlighted the lines where changes are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'message',\n",
       " 'id',\n",
       " 'date',\n",
       " '1993',\n",
       " 'path',\n",
       " 'from',\n",
       " 'gmt',\n",
       " 'subject',\n",
       " 'sender',\n",
       " 'references',\n",
       " 're',\n",
       " 'newsgroups',\n",
       " 'xref',\n",
       " 'organization',\n",
       " 'lines',\n",
       " 'writes',\n",
       " 'jan',\n",
       " 'feb',\n",
       " 'mar',\n",
       " 'apr',\n",
       " 'may',\n",
       " 'jun',\n",
       " 'jul',\n",
       " 'aug',\n",
       " 'sep',\n",
       " 'oct',\n",
       " 'nov',\n",
       " 'dec',\n",
       " 'mon',\n",
       " 'tue',\n",
       " 'wed',\n",
       " 'thu',\n",
       " 'fri',\n",
       " 'sat',\n",
       " 'sun',\n",
       " 'max',\n",
       " 'ax',\n",
       " 've',\n",
       " 'll',\n",
       " 's',\n",
       " 'reply',\n",
       " 'm',\n",
       " 'd',\n",
       " 'distribution',\n",
       " 'keywords',\n",
       " '1st',\n",
       " 'appears',\n",
       " '',\n",
       " ' many',\n",
       " ' ',\n",
       " '',\n",
       " 'followup',\n",
       " 'yet',\n",
       " 'ed',\n",
       " 'faq',\n",
       " 'good',\n",
       " 'well',\n",
       " 'yet',\n",
       " 'him',\n",
       " 'smuch',\n",
       " 'although',\n",
       " 'shall',\n",
       " 'without',\n",
       " 'goes',\n",
       " 'must',\n",
       " 'often',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'without',\n",
       " 'also',\n",
       " 'though',\n",
       " 'therefore',\n",
       " 'shall']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------------- This will represent the changes\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "l = ['message','id','date','1993','path','from','gmt','subject','sender','references','re','newsgroups','xref',\n",
    "      'organization','lines','writes',\n",
    "      'jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun',\n",
    "      'max','ax','ve','ll','s','reply','m','d',\n",
    "      'distribution','keywords','1st','appears','',' '\n",
    "      'many',' ','','followup','yet','ed',\n",
    "      'faq','good','well','yet','him','s'\n",
    "      'much','although','shall','without','goes','must','often',\n",
    "      'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t',\n",
    "      'without','also','though','therefore',\n",
    "      'shall']\n",
    "\n",
    "stop_words.extend(l)\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc', 'total_data'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "path_20_newsgrp = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Training Data)/20_newsgroups'\n",
    "categories = os.listdir(path_20_newsgrp) \n",
    "\n",
    "count = {}\n",
    "\n",
    "total_files = 0  \n",
    "total = 0        \n",
    "\n",
    "for category in categories:\n",
    "    d = {}\n",
    "    path_category = path_20_newsgrp + '/' + category   \n",
    "    files = os.listdir(path_category)\n",
    "    for file_id in files:\n",
    "        total += 1\n",
    "        file = open(path_category + '/' + file_id,'r',encoding=\"ISO-8859-1\")  \n",
    "        for line in file:\n",
    "            line = re.sub(\"[^a-zA-Z ]\",\" \",line)         #-------------------------------------------------------------------\n",
    "            string = str(line)  \n",
    "            words = string.strip().split(' ')        \n",
    "            if(len(words) == 1 and words[0] == ''):\n",
    "                continue\n",
    "            for word in words:\n",
    "                word = word.lower()                      #-------------------------------------------------------------------\n",
    "                if(word not in stop_words):          \n",
    "                    if(word not in d):               \n",
    "                        d[word] = 1                  \n",
    "                    else:\n",
    "                        d[word] += 1                 \n",
    "    \n",
    "    \n",
    "    d['total_count'] = total                         \n",
    "    total_files += total                             \n",
    "    total = 0                                        \n",
    "    count[category] = d                              \n",
    "\n",
    "\n",
    "count['total_data'] = total_files                    \n",
    "    \n",
    "count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in  alt.atheism : 15998\n",
      "Number of words in  comp.graphics : 16326\n",
      "Number of words in  comp.os.ms-windows.misc : 29338\n",
      "Number of words in  comp.sys.ibm.pc.hardware : 12975\n",
      "Number of words in  comp.sys.mac.hardware : 12700\n",
      "Number of words in  comp.windows.x : 19330\n",
      "Number of words in  misc.forsale : 13763\n",
      "Number of words in  rec.autos : 14150\n",
      "Number of words in  rec.motorcycles : 13601\n",
      "Number of words in  rec.sport.baseball : 12508\n",
      "Number of words in  rec.sport.hockey : 13965\n",
      "Number of words in  sci.crypt : 17009\n",
      "Number of words in  sci.electronics : 14043\n",
      "Number of words in  sci.med : 20123\n",
      "Number of words in  sci.space : 17995\n",
      "Number of words in  soc.religion.christian : 17298\n",
      "Number of words in  talk.politics.guns : 18274\n",
      "Number of words in  talk.politics.mideast : 20611\n",
      "Number of words in  talk.politics.misc : 19390\n",
      "Number of words in  talk.religion.misc : 17734\n",
      "\n",
      "Total Number of words :  337131\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "\n",
    "for key in count:\n",
    "    if(key == 'total_data'):\n",
    "        continue\n",
    "    count_ = len(count[key].keys()) - 1\n",
    "    print('Number of words in ',key,':',count_)\n",
    "    total_words += count_        \n",
    "\n",
    "print()    \n",
    "print('Total Number of words : ',total_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in  alt.atheism (with frequency greater than 20) : 1878\n",
      "Number of words in  comp.graphics (with frequency greater than 20) : 1778\n",
      "Number of words in  comp.os.ms-windows.misc (with frequency greater than 20) : 1987\n",
      "Number of words in  comp.sys.ibm.pc.hardware (with frequency greater than 20) : 1309\n",
      "Number of words in  comp.sys.mac.hardware (with frequency greater than 20) : 1246\n",
      "Number of words in  comp.windows.x (with frequency greater than 20) : 1832\n",
      "Number of words in  misc.forsale (with frequency greater than 20) : 1069\n",
      "Number of words in  rec.autos (with frequency greater than 20) : 1511\n",
      "Number of words in  rec.motorcycles (with frequency greater than 20) : 1435\n",
      "Number of words in  rec.sport.baseball (with frequency greater than 20) : 1508\n",
      "Number of words in  rec.sport.hockey (with frequency greater than 20) : 1705\n",
      "Number of words in  sci.crypt (with frequency greater than 20) : 1934\n",
      "Number of words in  sci.electronics (with frequency greater than 20) : 1501\n",
      "Number of words in  sci.med (with frequency greater than 20) : 1936\n",
      "Number of words in  sci.space (with frequency greater than 20) : 1973\n",
      "Number of words in  soc.religion.christian (with frequency greater than 20) : 1893\n",
      "Number of words in  talk.politics.guns (with frequency greater than 20) : 2076\n",
      "Number of words in  talk.politics.mideast (with frequency greater than 20) : 2702\n",
      "Number of words in  talk.politics.misc (with frequency greater than 20) : 2412\n",
      "Number of words in  talk.religion.misc (with frequency greater than 20) : 2013\n",
      "\n",
      "Total Number of words (with frequency greater than 20) : 35698\n"
     ]
    }
   ],
   "source": [
    "for key1 in count:\n",
    "    if(key1 == 'total_data'):\n",
    "        continue\n",
    "    d = count[key1]             \n",
    "    d1 = {}                     \n",
    "    for key2 in d:\n",
    "        if(key2 == 'total_count'):\n",
    "            d1['total_count'] = d['total_count']    \n",
    "            continue\n",
    "        if(d[key2] < 20):\n",
    "            continue                  \n",
    "        else:\n",
    "            d1[key2] = d[key2]        \n",
    "    count[key1] = d1                  \n",
    "    d.clear()                         \n",
    "\n",
    "total_words = 0   \n",
    "for key in count:\n",
    "    if(key == 'total_data'):\n",
    "        continue\n",
    "    count_ = len(count[key].keys()) - 1\n",
    "    print('Number of words in ',key,'(with frequency greater than 20)',':',count_)\n",
    "    total_words += count_        \n",
    "\n",
    "print()    \n",
    "print('Total Number of words (with frequency greater than 20) :',total_words)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35698"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = []      \n",
    "\n",
    "for key1 in count:\n",
    "    if(key1 == 'total_data'):\n",
    "        continue\n",
    "    d = count[key1]\n",
    "    for key2 in d:\n",
    "        if(key2 == 'total_count'):\n",
    "            continue\n",
    "        features.append(key2)           \n",
    "\n",
    "len(features)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10511 10511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cantaloupe',\n",
       " 'srv',\n",
       " 'cs',\n",
       " 'cmu',\n",
       " 'edu',\n",
       " 'alt',\n",
       " 'atheism',\n",
       " 'moderated',\n",
       " 'news',\n",
       " 'answers',\n",
       " 'crabapple',\n",
       " 'bb',\n",
       " 'andrew',\n",
       " 'sei',\n",
       " 'cis',\n",
       " 'ohio',\n",
       " 'state',\n",
       " 'acs',\n",
       " 'usenet',\n",
       " 'ins',\n",
       " 'cwru',\n",
       " 'agate',\n",
       " 'spool',\n",
       " 'mu',\n",
       " 'uunet',\n",
       " 'pipex',\n",
       " 'ibmpcug',\n",
       " 'mantis',\n",
       " 'mathew',\n",
       " 'co',\n",
       " 'uk',\n",
       " 'atheist',\n",
       " 'resources',\n",
       " 'summary',\n",
       " 'books',\n",
       " 'anything',\n",
       " 'related',\n",
       " 'fiction',\n",
       " 'world',\n",
       " 'consultants',\n",
       " 'cambridge',\n",
       " 'request',\n",
       " 'mit',\n",
       " 'name',\n",
       " 'last',\n",
       " 'version',\n",
       " 'organizations',\n",
       " 'usa',\n",
       " 'freedom',\n",
       " 'religion',\n",
       " 'foundation',\n",
       " 'darwin',\n",
       " 'fish',\n",
       " 'available',\n",
       " 'us',\n",
       " 'write',\n",
       " 'box',\n",
       " 'evolution',\n",
       " 'sell',\n",
       " 'like',\n",
       " 'ones',\n",
       " 'christians',\n",
       " 'stick',\n",
       " 'word',\n",
       " 'written',\n",
       " 'ca',\n",
       " 'people',\n",
       " 'san',\n",
       " 'area',\n",
       " 'get',\n",
       " 'try',\n",
       " 'netcom',\n",
       " 'com',\n",
       " 'net',\n",
       " 'go',\n",
       " 'directly',\n",
       " 'price',\n",
       " 'american',\n",
       " 'press',\n",
       " 'various',\n",
       " 'bible',\n",
       " 'biblical',\n",
       " 'contradictions',\n",
       " 'one',\n",
       " 'book',\n",
       " 'w',\n",
       " 'nd',\n",
       " 'contains',\n",
       " 'based',\n",
       " 'king',\n",
       " 'james',\n",
       " 'austin',\n",
       " 'including',\n",
       " 'holy',\n",
       " 'see',\n",
       " 'east',\n",
       " 'new',\n",
       " 'york',\n",
       " 'address',\n",
       " 'americans',\n",
       " 'black',\n",
       " 'secular',\n",
       " 'history',\n",
       " 'national',\n",
       " 'society',\n",
       " 'high',\n",
       " 'british',\n",
       " 'south',\n",
       " 'place',\n",
       " 'ethical',\n",
       " 'passage',\n",
       " 'rh',\n",
       " 'founded',\n",
       " 'germany',\n",
       " 'v',\n",
       " 'thomas',\n",
       " 'compromise',\n",
       " 'short',\n",
       " 'story',\n",
       " 'proof',\n",
       " 'exists',\n",
       " 'events',\n",
       " 'living',\n",
       " 'dead',\n",
       " 'gods',\n",
       " 'miller',\n",
       " 'post',\n",
       " 'atomic',\n",
       " 'lives',\n",
       " 'paper',\n",
       " 'white',\n",
       " 'set',\n",
       " 'states',\n",
       " 'church',\n",
       " 'example',\n",
       " 'anyone',\n",
       " 'produce',\n",
       " 'describe',\n",
       " 'use',\n",
       " 'atoms',\n",
       " 'wrote',\n",
       " 'many',\n",
       " 'philosophical',\n",
       " 'thought',\n",
       " 'stories',\n",
       " 'times',\n",
       " 'sf',\n",
       " 'truth',\n",
       " 'rather',\n",
       " 'technology',\n",
       " 'believed',\n",
       " 'sort',\n",
       " 'god',\n",
       " 'following',\n",
       " 'group',\n",
       " 'earth',\n",
       " 'women',\n",
       " 'raise',\n",
       " 'faith',\n",
       " 'amusing',\n",
       " 'death',\n",
       " 'description',\n",
       " 'christianity',\n",
       " 'reality',\n",
       " 'brain',\n",
       " 'pink',\n",
       " 'possibly',\n",
       " 'divine',\n",
       " 'origin',\n",
       " 'making',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'another',\n",
       " 'system',\n",
       " 'unfortunately',\n",
       " 'man',\n",
       " 'whose',\n",
       " 'easy',\n",
       " 'premise',\n",
       " 'take',\n",
       " 'charge',\n",
       " 'nation',\n",
       " 'right',\n",
       " 'life',\n",
       " 'live',\n",
       " 'christian',\n",
       " 'bank',\n",
       " 'closed',\n",
       " 'used',\n",
       " 'legal',\n",
       " 'old',\n",
       " 'writing',\n",
       " 'difficult',\n",
       " 'first',\n",
       " 'authors',\n",
       " 'somewhat',\n",
       " 'work',\n",
       " 'however',\n",
       " 'probably',\n",
       " 'worth',\n",
       " 'reading',\n",
       " 'know',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'true',\n",
       " 'non',\n",
       " 'peter',\n",
       " 'de',\n",
       " 'christ',\n",
       " 'seems',\n",
       " 'even',\n",
       " 'catholic',\n",
       " 'etc',\n",
       " 'german',\n",
       " 'translation',\n",
       " 'die',\n",
       " 'des',\n",
       " 'michael',\n",
       " 'justification',\n",
       " 'temple',\n",
       " 'university',\n",
       " 'necessarily',\n",
       " 'belief',\n",
       " 'existence',\n",
       " 'positive',\n",
       " 'includes',\n",
       " 'great',\n",
       " 'arguments',\n",
       " 'particular',\n",
       " 'theists',\n",
       " 'case',\n",
       " 'best',\n",
       " 'origins',\n",
       " 'america',\n",
       " 'way',\n",
       " 'whether',\n",
       " 'agnostic',\n",
       " 'became',\n",
       " 'alternative',\n",
       " 'view',\n",
       " 'period',\n",
       " 'considering',\n",
       " 'particularly',\n",
       " 'neither',\n",
       " 'religious',\n",
       " 'intellectual',\n",
       " 'single',\n",
       " 'idea',\n",
       " 'x',\n",
       " 'george',\n",
       " 'thoughts',\n",
       " 'dictionary',\n",
       " 'kind',\n",
       " 'statements',\n",
       " 'writings',\n",
       " 'explicitly',\n",
       " 'present',\n",
       " 'person',\n",
       " 'philosophy',\n",
       " 'opinions',\n",
       " 'popular',\n",
       " 'observations',\n",
       " 'quite',\n",
       " 'number',\n",
       " 'men',\n",
       " 'think',\n",
       " 'views',\n",
       " 'richard',\n",
       " 'second',\n",
       " 'theism',\n",
       " 'reason',\n",
       " 'attempts',\n",
       " 'rely',\n",
       " 'upon',\n",
       " 'th',\n",
       " 'century',\n",
       " 'western',\n",
       " 'values',\n",
       " 'simple',\n",
       " 'makes',\n",
       " 'attempt',\n",
       " 'al',\n",
       " 'moral',\n",
       " 'concept',\n",
       " 'beyond',\n",
       " 'rational',\n",
       " 'read',\n",
       " 'less',\n",
       " 'better',\n",
       " 'works',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'murder',\n",
       " 'looks',\n",
       " 'ancient',\n",
       " 'day',\n",
       " 'library',\n",
       " 'evil',\n",
       " 'morality',\n",
       " 'mind',\n",
       " 'believer',\n",
       " 'study',\n",
       " 'become',\n",
       " 'effect',\n",
       " 'small',\n",
       " 'mail',\n",
       " 'server',\n",
       " 'articles',\n",
       " 'files',\n",
       " 'information',\n",
       " 'send',\n",
       " 'saying',\n",
       " 'help',\n",
       " 'back',\n",
       " 'fs',\n",
       " 'ece',\n",
       " 'europa',\n",
       " 'eng',\n",
       " 'gtefsd',\n",
       " 'howland',\n",
       " 'reston',\n",
       " 'ans',\n",
       " 'introduction',\n",
       " 'please',\n",
       " 'file',\n",
       " 'posting',\n",
       " 'article',\n",
       " 'provide',\n",
       " 'general',\n",
       " 'tried',\n",
       " 'possible',\n",
       " 'regarding',\n",
       " 'issues',\n",
       " 'always',\n",
       " 'remember',\n",
       " 'viewpoint',\n",
       " 'would',\n",
       " 'conclusions',\n",
       " 'relevant',\n",
       " 'sense',\n",
       " 'presented',\n",
       " 'theist',\n",
       " 'questions',\n",
       " 'asked',\n",
       " 'since',\n",
       " 'newsgroup',\n",
       " 'created',\n",
       " 'answered',\n",
       " 'note',\n",
       " 'towards',\n",
       " 'actually',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'religions',\n",
       " 'islam',\n",
       " 'much',\n",
       " 'discussion',\n",
       " 'apply',\n",
       " 'absence',\n",
       " 'atheists',\n",
       " 'believe',\n",
       " 'exist',\n",
       " 'former',\n",
       " 'referred',\n",
       " 'weak',\n",
       " 'position',\n",
       " 'latter',\n",
       " 'strong',\n",
       " 'important',\n",
       " 'difference',\n",
       " 'two',\n",
       " 'fall',\n",
       " 'assuming',\n",
       " 'others',\n",
       " 'specific',\n",
       " 'thing',\n",
       " 'believing',\n",
       " 'proposition',\n",
       " 'means',\n",
       " 'something',\n",
       " 'false',\n",
       " 'simply',\n",
       " 'term',\n",
       " 'defined',\n",
       " 'someone',\n",
       " 'things',\n",
       " 'cause',\n",
       " 'thus',\n",
       " 'believes',\n",
       " 'cannot',\n",
       " 'words',\n",
       " 'language',\n",
       " 'point',\n",
       " 'fact',\n",
       " 'mean',\n",
       " 'referring',\n",
       " 'meaning',\n",
       " 'say',\n",
       " 'certainly',\n",
       " 'science',\n",
       " 'find',\n",
       " 'universe',\n",
       " 'basis',\n",
       " 'ask',\n",
       " 'feel',\n",
       " 'major',\n",
       " 'essentially',\n",
       " 'self',\n",
       " 'contradictory',\n",
       " 'logically',\n",
       " 'impossible',\n",
       " 'could',\n",
       " 'evidence',\n",
       " 'prove',\n",
       " 'counter',\n",
       " 'examples',\n",
       " 'statement',\n",
       " 'course',\n",
       " 'rules',\n",
       " 'similarly',\n",
       " 'matter',\n",
       " 'debate',\n",
       " 'moment',\n",
       " 'still',\n",
       " 'reasons',\n",
       " 'assume',\n",
       " 'show',\n",
       " 'assumption',\n",
       " 'question',\n",
       " 'showing',\n",
       " 'require',\n",
       " 'places',\n",
       " 'might',\n",
       " 'found',\n",
       " 'problem',\n",
       " 'generally',\n",
       " 'accepted',\n",
       " 'unless',\n",
       " 'follow',\n",
       " 'rule',\n",
       " 'time',\n",
       " 'test',\n",
       " 'usually',\n",
       " 'claim',\n",
       " 'instead',\n",
       " 'claims',\n",
       " 'described',\n",
       " 'followers',\n",
       " 'practice',\n",
       " 'close',\n",
       " 'every',\n",
       " 'really',\n",
       " 'effects',\n",
       " 'hence',\n",
       " 'argue',\n",
       " 'importance',\n",
       " 'easily',\n",
       " 'surely',\n",
       " 'today',\n",
       " 'physical',\n",
       " 'caused',\n",
       " 'otherwise',\n",
       " 'ok',\n",
       " 'common',\n",
       " 'game',\n",
       " 'follows',\n",
       " 'points',\n",
       " 'prepared',\n",
       " 'accept',\n",
       " 'along',\n",
       " 'agreed',\n",
       " 'uses',\n",
       " 'original',\n",
       " 'definitions',\n",
       " 'agree',\n",
       " 'seen',\n",
       " 'apparently',\n",
       " 'tend',\n",
       " 'play',\n",
       " 'answer',\n",
       " 'depends',\n",
       " 'meant',\n",
       " 'power',\n",
       " 'especially',\n",
       " 'worship',\n",
       " 'according',\n",
       " 'definition',\n",
       " 'result',\n",
       " 'human',\n",
       " 'behaviour',\n",
       " 'act',\n",
       " 'entirely',\n",
       " 'clear',\n",
       " 'necessary',\n",
       " 'beliefs',\n",
       " 'data',\n",
       " 'experience',\n",
       " 'doubt',\n",
       " 'assumed',\n",
       " 'laws',\n",
       " 'physics',\n",
       " 'basic',\n",
       " 'ideas',\n",
       " 'called',\n",
       " 'acts',\n",
       " 'almost',\n",
       " 'everything',\n",
       " 'said',\n",
       " 'refer',\n",
       " 'complete',\n",
       " 'certain',\n",
       " 'individual',\n",
       " 'scientists',\n",
       " 'claiming',\n",
       " 'fit',\n",
       " 'lack',\n",
       " 'anti',\n",
       " 'everyone',\n",
       " 'either',\n",
       " 'cut',\n",
       " 'believers',\n",
       " 'speak',\n",
       " 'let',\n",
       " 'mention',\n",
       " 'except',\n",
       " 'perhaps',\n",
       " 'friends',\n",
       " 'part',\n",
       " 'acceptable',\n",
       " 'countries',\n",
       " 'made',\n",
       " 'little',\n",
       " 'outside',\n",
       " 'free',\n",
       " 'came',\n",
       " 'stalin',\n",
       " 'took',\n",
       " 'control',\n",
       " 'destroy',\n",
       " 'order',\n",
       " 'population',\n",
       " 'matters',\n",
       " 'business',\n",
       " 'government',\n",
       " 'individuals',\n",
       " 'concerned',\n",
       " 'allow',\n",
       " 'principle',\n",
       " 'concerning',\n",
       " 'nature',\n",
       " 'responsible',\n",
       " 'political',\n",
       " 'spending',\n",
       " 'long',\n",
       " 'happy',\n",
       " 'care',\n",
       " 'pray',\n",
       " 'ignore',\n",
       " 'told',\n",
       " 'need',\n",
       " 'public',\n",
       " 'event',\n",
       " 'family',\n",
       " 'reasonable',\n",
       " 'mentioned',\n",
       " 'object',\n",
       " 'purpose',\n",
       " 'sake',\n",
       " 'theistic',\n",
       " 'seem',\n",
       " 'obvious',\n",
       " 'excuse',\n",
       " 'strange',\n",
       " 'rarely',\n",
       " 'force',\n",
       " 'choose',\n",
       " 'call',\n",
       " 'whole',\n",
       " 'conclude',\n",
       " 'listen',\n",
       " 'prefer',\n",
       " 'copy',\n",
       " 'around',\n",
       " 'arguing',\n",
       " 'several',\n",
       " 'define',\n",
       " 'wrong',\n",
       " 'within',\n",
       " 'humans',\n",
       " 'social',\n",
       " 'animals',\n",
       " 'successful',\n",
       " 'enough',\n",
       " 'immoral',\n",
       " 'purposes',\n",
       " 'natural',\n",
       " 'happens',\n",
       " 'justify',\n",
       " 'actions',\n",
       " 'full',\n",
       " 'jesus',\n",
       " 'save',\n",
       " 'shown',\n",
       " 'eternal',\n",
       " 'ever',\n",
       " 'quote',\n",
       " 'court',\n",
       " 'wisconsin',\n",
       " 'mass',\n",
       " 'behavior',\n",
       " 'born',\n",
       " 'experiences',\n",
       " 'done',\n",
       " 'percent',\n",
       " 'sex',\n",
       " 'yes',\n",
       " 'explained',\n",
       " 'least',\n",
       " 'held',\n",
       " 'code',\n",
       " 'imply',\n",
       " 'ought',\n",
       " 'seeing',\n",
       " 'supernatural',\n",
       " 'observed',\n",
       " 'ignorance',\n",
       " 'choice',\n",
       " 'majority',\n",
       " 'sometimes',\n",
       " 'considered',\n",
       " 'decision',\n",
       " 'reject',\n",
       " 'want',\n",
       " 'nobody',\n",
       " 'big',\n",
       " 'figure',\n",
       " 'able',\n",
       " 'merely',\n",
       " 'wants',\n",
       " 'approach',\n",
       " 'decide',\n",
       " 'seriously',\n",
       " 'possibility',\n",
       " 'trying',\n",
       " 'reach',\n",
       " 'open',\n",
       " 'comments',\n",
       " 'looking',\n",
       " 'properly',\n",
       " 'likely',\n",
       " 'wish',\n",
       " 'give',\n",
       " 'sincere',\n",
       " 'willing',\n",
       " 'basically',\n",
       " 'telling',\n",
       " 'completely',\n",
       " 'gives',\n",
       " 'goals',\n",
       " 'hope',\n",
       " 'mark',\n",
       " 'look',\n",
       " 'put',\n",
       " 'asking',\n",
       " 'silly',\n",
       " 'ways',\n",
       " 'spiritual',\n",
       " 'level',\n",
       " 'drink',\n",
       " 'sound',\n",
       " 'face',\n",
       " 'end',\n",
       " 'consider',\n",
       " 'hard',\n",
       " 'years',\n",
       " 'supposed',\n",
       " 'thousands',\n",
       " 'unlikely',\n",
       " 'proved',\n",
       " 'future',\n",
       " 'stop',\n",
       " 'similar',\n",
       " 'nothing',\n",
       " 'real',\n",
       " 'harm',\n",
       " 'else',\n",
       " 'burden',\n",
       " 'money',\n",
       " 'effort',\n",
       " 'imagine',\n",
       " 'died',\n",
       " 'third',\n",
       " 'known',\n",
       " 'children',\n",
       " 'claimed',\n",
       " 'tell',\n",
       " 'groups',\n",
       " 'convinced',\n",
       " 'kill',\n",
       " 'dogma',\n",
       " 'maybe',\n",
       " 'support',\n",
       " 'intended',\n",
       " 'interpretation',\n",
       " 'sorry',\n",
       " 'obviously',\n",
       " 'nonsense',\n",
       " 'hold',\n",
       " 'pointed',\n",
       " 'start',\n",
       " 'valid',\n",
       " 'assertion',\n",
       " 'correct',\n",
       " 'primitive',\n",
       " 'societies',\n",
       " 'allows',\n",
       " 'deal',\n",
       " 'phenomena',\n",
       " 'understand',\n",
       " 'perfectly',\n",
       " 'started',\n",
       " 'explain',\n",
       " 'philosopher',\n",
       " 'besides',\n",
       " 'already',\n",
       " 'realize',\n",
       " 'leaders',\n",
       " 'special',\n",
       " 'speaking',\n",
       " 'lie',\n",
       " 'far',\n",
       " 'theory',\n",
       " 'minds',\n",
       " 'analogy',\n",
       " 'avoid',\n",
       " 'using',\n",
       " 'keep',\n",
       " 'disease',\n",
       " 'fundamental',\n",
       " 'change',\n",
       " 'thanks',\n",
       " 'taking',\n",
       " 'z',\n",
       " 'usc',\n",
       " 'sdd',\n",
       " 'hp',\n",
       " 'nigel',\n",
       " 'msen',\n",
       " 'yale',\n",
       " 'ira',\n",
       " 'uka',\n",
       " 'dfn',\n",
       " 'tubsibr',\n",
       " 'dbstu',\n",
       " 'rz',\n",
       " 'tu',\n",
       " 'bs',\n",
       " 'benedikt',\n",
       " 'rosenau',\n",
       " 'gospel',\n",
       " 'ba',\n",
       " 'postnntp',\n",
       " 'ibr',\n",
       " 'mr',\n",
       " 'nntp',\n",
       " 'inews',\n",
       " 'entry',\n",
       " 'technical',\n",
       " 'braunschweig',\n",
       " 'mimsy',\n",
       " 'umd',\n",
       " 'mangoe',\n",
       " 'charley',\n",
       " 'wingate',\n",
       " 'john',\n",
       " 'theology',\n",
       " 'luke',\n",
       " 'content',\n",
       " 'form',\n",
       " 'argument',\n",
       " 'quotes',\n",
       " 'appear',\n",
       " 'matthew',\n",
       " 'knowledge',\n",
       " 'source',\n",
       " 'knew',\n",
       " 'texts',\n",
       " 'age',\n",
       " 'usual',\n",
       " 'explanation',\n",
       " 'says',\n",
       " 'text',\n",
       " 'overwhelming',\n",
       " 'earlier',\n",
       " 'interesting',\n",
       " 'got',\n",
       " 'early',\n",
       " 'putting',\n",
       " 'rest',\n",
       " 'three',\n",
       " 'exactly',\n",
       " 'base',\n",
       " 'account',\n",
       " 'paul',\n",
       " 'letter',\n",
       " 'together',\n",
       " 'fine',\n",
       " 'step',\n",
       " 'bad',\n",
       " 'gets',\n",
       " 'generation',\n",
       " 'sources',\n",
       " 'lost',\n",
       " 'material',\n",
       " 'verse',\n",
       " 'pretty',\n",
       " 'clearly',\n",
       " 'later',\n",
       " 'constitution',\n",
       " 'wupost',\n",
       " 'rusnews',\n",
       " 'newsreader',\n",
       " 'recently',\n",
       " 'none',\n",
       " 'soul',\n",
       " 'school',\n",
       " 'somewhere',\n",
       " 'issue',\n",
       " 'next',\n",
       " 'heard',\n",
       " 'defend',\n",
       " 'sounds',\n",
       " 'run',\n",
       " 'soc',\n",
       " 'motss',\n",
       " 'rec',\n",
       " 'scouting',\n",
       " 'watson',\n",
       " 'ibm',\n",
       " 'princeton',\n",
       " 'boy',\n",
       " 'cb',\n",
       " 'att',\n",
       " 'org',\n",
       " 'research',\n",
       " 'bob',\n",
       " 'worse',\n",
       " 'policy',\n",
       " 'rights',\n",
       " 'somebody',\n",
       " 'contradiction',\n",
       " 'saw',\n",
       " 'witnesses',\n",
       " 'ksu',\n",
       " 'batman',\n",
       " 'bmd',\n",
       " 'trw',\n",
       " 'jbrown',\n",
       " 'perfect',\n",
       " 'terms',\n",
       " 'leads',\n",
       " 'coming',\n",
       " 'mouth',\n",
       " 'shows',\n",
       " 'away',\n",
       " 'ability',\n",
       " 'alternatives',\n",
       " 'error',\n",
       " 'opinion',\n",
       " 'specifically',\n",
       " 'occur',\n",
       " 'happen',\n",
       " 'omniscient',\n",
       " 'conscious',\n",
       " 'beings',\n",
       " 'omnipotent',\n",
       " 'left',\n",
       " 'greater',\n",
       " 'sin',\n",
       " 'knowing',\n",
       " 'truly',\n",
       " 'consistent',\n",
       " 'deletion',\n",
       " 'disagree',\n",
       " 'describes',\n",
       " 'logic',\n",
       " 'amount',\n",
       " 'fair',\n",
       " 'lot',\n",
       " 'interested',\n",
       " 'noc',\n",
       " 'near',\n",
       " 'olivea',\n",
       " 'sgigate',\n",
       " 'sgiblab',\n",
       " 'adagio',\n",
       " 'panasonic',\n",
       " 'caltech',\n",
       " 'keith',\n",
       " 'cco',\n",
       " 'allan',\n",
       " 'schneider',\n",
       " 'pi',\n",
       " 'gap',\n",
       " 'blaze',\n",
       " 'jhu',\n",
       " 'california',\n",
       " 'institute',\n",
       " 'pasadena',\n",
       " 'host',\n",
       " 'punisher',\n",
       " 'arromdee',\n",
       " 'jyusenkyou',\n",
       " 'ken',\n",
       " 'motto',\n",
       " 'war',\n",
       " 'required',\n",
       " 'sol',\n",
       " 'ctr',\n",
       " 'columbia',\n",
       " 'gov',\n",
       " 'bu',\n",
       " 'jaeger',\n",
       " 'buphy',\n",
       " 'gregg',\n",
       " 'muslims',\n",
       " 'tm',\n",
       " 'muslim',\n",
       " 'prophet',\n",
       " 'phenomenon',\n",
       " 'islamic',\n",
       " 'principles',\n",
       " 'attack',\n",
       " 'situation',\n",
       " 'standards',\n",
       " 'zaphod',\n",
       " 'mps',\n",
       " 'fido',\n",
       " 'asd',\n",
       " 'sgi',\n",
       " 'reference',\n",
       " 'line',\n",
       " 'livesey',\n",
       " 'solntze',\n",
       " 'wpd',\n",
       " 'jon',\n",
       " 'confusion',\n",
       " 'objective',\n",
       " 'admit',\n",
       " 'objectively',\n",
       " 'requires',\n",
       " 'goal',\n",
       " 'morals',\n",
       " 'survival',\n",
       " 'species',\n",
       " 'liberty',\n",
       " 'unto',\n",
       " 'killing',\n",
       " 'execution',\n",
       " 'innocent',\n",
       " 'capital',\n",
       " 'punishment',\n",
       " 'fail',\n",
       " 'never',\n",
       " 'current',\n",
       " 'personal',\n",
       " 'arabia',\n",
       " 'implies',\n",
       " 'reasoning',\n",
       " 'ass',\n",
       " 'mvs',\n",
       " 'psuvm',\n",
       " 'psu',\n",
       " 'pa',\n",
       " 'bmerh',\n",
       " 'bnr',\n",
       " 'kmr',\n",
       " 'po',\n",
       " 'ryan',\n",
       " 'stay',\n",
       " 'becomes',\n",
       " 'notion',\n",
       " 'ago',\n",
       " 'poster',\n",
       " 'mam',\n",
       " 'mouse',\n",
       " 'cmhnet',\n",
       " 'sandvik',\n",
       " 'kent',\n",
       " 'apple',\n",
       " 'newton',\n",
       " 'judge',\n",
       " 'subjectively',\n",
       " 'discuss',\n",
       " 'misc',\n",
       " 'albert',\n",
       " 'sabin',\n",
       " 'rambo',\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set = []  \n",
    "\n",
    "for i in features:\n",
    "    if(i not in feature_set):\n",
    "        feature_set.append(i)\n",
    "        \n",
    "print(len(feature_set),len(set(feature_set)))\n",
    "# feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling our predict function on the Testing data\n",
    "\n",
    "path_mini_news_group = 'C:/Users/Keerat/Desktop/CN Files/Text Classification (Testing Data)/mini_newsgroups'\n",
    "Y_predicted1 = predict(path_mini_news_group,feature_set,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on Testing Data using this approach : 0.6665\n"
     ]
    }
   ],
   "source": [
    "count_ = 0\n",
    "for i in range(len(Y_predicted1)):\n",
    "    if(Y_predicted1[i] == Y_test[i]):\n",
    "        count_ += 1\n",
    "print('Score on Testing Data using this approach :',count_/len(Y_predicted1))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.55      0.57      0.56        97\n",
      "           comp.graphics       0.78      0.51      0.61       154\n",
      " comp.os.ms-windows.misc       0.07      0.78      0.13         9\n",
      "comp.sys.ibm.pc.hardware       0.68      0.65      0.67       104\n",
      "   comp.sys.mac.hardware       0.60      0.65      0.62        93\n",
      "          comp.windows.x       0.71      0.72      0.72        98\n",
      "            misc.forsale       0.59      0.89      0.71        66\n",
      "               rec.autos       0.71      0.70      0.71       101\n",
      "         rec.motorcycles       0.62      0.89      0.73        70\n",
      "      rec.sport.baseball       0.72      0.80      0.76        90\n",
      "        rec.sport.hockey       0.72      0.94      0.81        77\n",
      "               sci.crypt       0.85      0.64      0.73       132\n",
      "         sci.electronics       0.72      0.64      0.68       113\n",
      "                 sci.med       0.92      0.81      0.86       114\n",
      "               sci.space       0.81      0.85      0.83        95\n",
      "  soc.religion.christian       0.88      0.56      0.68       158\n",
      "      talk.politics.guns       0.64      0.67      0.65        96\n",
      "   talk.politics.mideast       0.79      0.69      0.73       115\n",
      "      talk.politics.misc       0.75      0.41      0.53       185\n",
      "      talk.religion.misc       0.22      0.67      0.33        33\n",
      "\n",
      "               micro avg       0.67      0.67      0.67      2000\n",
      "               macro avg       0.67      0.70      0.65      2000\n",
      "            weighted avg       0.73      0.67      0.68      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_predicted1,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
